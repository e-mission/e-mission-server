{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from uuid import UUID\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# import logging\n",
                "# logging.basicConfig(level=logging.DEBUG)\n",
                "\n",
                "import emission.storage.timeseries.abstract_timeseries as esta\n",
                "import emission.storage.decorations.trip_queries as esdtq\n",
                "import emission.analysis.modelling.trip_model.run_model as eamtr\n",
                "from performance_eval import get_clf_metrics, cv_for_all_algs, PREDICTORS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### load data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_users = esta.TimeSeries.get_uuid_list()\n",
                "confirmed_trip_df_map = {}\n",
                "labeled_trip_df_map = {}\n",
                "expanded_labeled_trip_df_map = {}\n",
                "expanded_all_trip_df_map = {}\n",
                "ct_entry={}\n",
                "for u in all_users:\n",
                "    ts = esta.TimeSeries.get_time_series(u)\n",
                "    ct_entry[u]=eamtr._get_training_data(u,None)\n",
                "    ct_df = ts.to_data_df(\"analysis/confirmed_trip\",ct_entry[u])\n",
                "    confirmed_trip_df_map[u] = ct_df\n",
                "    labeled_trip_df_map[u] = esdtq.filter_labeled_trips(ct_df)\n",
                "    expanded_labeled_trip_df_map[u] = esdtq.expand_userinputs(\n",
                "        labeled_trip_df_map[u])\n",
                "    expanded_all_trip_df_map[u] = esdtq.expand_userinputs(\n",
                "        confirmed_trip_df_map[u])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "check how many labeled/unlabeled trips there are:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_trips_df = pd.DataFrame(\n",
                "    [[u, len(confirmed_trip_df_map[u]),\n",
                "      len(labeled_trip_df_map[u])] for u in all_users],\n",
                "    columns=[\"user_id\", \"all_trips\", \"labeled_trips\"])\n",
                "\n",
                "all_trips = n_trips_df.all_trips.sum()\n",
                "labeled_trips = n_trips_df.labeled_trips.sum()\n",
                "unlabeled_trips = all_trips - labeled_trips\n",
                "n_users = len(n_trips_df)\n",
                "\n",
                "print('{} ({:.2f}%) unlabeled, {} ({:.2f}%) labeled, {} total trips'.format(\n",
                "    unlabeled_trips, unlabeled_trips / all_trips, labeled_trips,\n",
                "    labeled_trips / all_trips, all_trips))\n",
                "\n",
                "n_users_too_few_trips = len(n_trips_df[n_trips_df.labeled_trips < 5])\n",
                "print(\n",
                "    '{}/{} ({:.2f}%) users have less than 5 labeled trips and cannot do cross-validation'\n",
                "    .format(n_users_too_few_trips, n_users, n_users_too_few_trips / n_users))\n",
                "\n",
                "trips_for_crossval = n_trips_df[\n",
                "    n_trips_df.labeled_trips >= 5].labeled_trips.sum()\n",
                "print('{} trips usable for 5-fold cross-validation from {}/{} ({:.2f}%) users'.\n",
                "      format(trips_for_crossval, n_users - n_users_too_few_trips, n_users,\n",
                "             (n_users - n_users_too_few_trips) / n_users))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### evaluate performance in aggregate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following cell will load the cross-validation results for the listed models. (Parameters for the models being tested can be found in tour_model_eval/performance_eval.py)\n",
                "\n",
                "If the cross-validation results for a model have already been generated, it will attempt to load it from the csv file to avoid the time-consuming process of re-running it. Otherwise, it will run the cross-validation from scratch. (This feature can be toggled with the override_prior_runs parameter - if True, it will ignore existing csv's and re-run from scratch.)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "WARNING! The following cell will take *insanely long* to run - as in, potentially up to 2 days - largely due to the fixed-width clustering algorithms. If you don't care about those, I suggest removing them from the list of models to evaluate. The other models took me ~20min each to run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load in all runs\n",
                "model_names = list(PREDICTORS.keys())\n",
                "cv_results = cv_for_all_algs(\n",
                "    ct_entry,\n",
                "    uuid_list=all_users,\n",
                "    expanded_trip_df_map=expanded_labeled_trip_df_map,\n",
                "    model_names=model_names,\n",
                "    override_prior_runs=False,\n",
                "    raise_errors=False,\n",
                "    random_state=42,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# store results for all models in a nice dataframe\n",
                "all_model_results = {\n",
                "    ('model_name', ''): [],\n",
                "    ('trips without prediction', 'purpose'): [],\n",
                "    ('trips without prediction', 'mode'): [],\n",
                "    ('trips without prediction', 'replaced'): [],\n",
                "    ('accuracy overall', 'purpose'): [],\n",
                "    ('accuracy overall', 'mode'): [],\n",
                "    ('accuracy overall', 'replaced'): [],\n",
                "    ('accuracy of trips w predictions', 'purpose'): [],\n",
                "    ('accuracy of trips w predictions', 'mode'): [],\n",
                "    ('accuracy of trips w predictions', 'replaced'): [],\n",
                "    ('f1 weighted', 'purpose'): [],\n",
                "    ('f1 weighted', 'mode'): [],\n",
                "    ('f1 weighted', 'replaced'): [],\n",
                "}\n",
                "\n",
                "purpose_results = {}\n",
                "\n",
                "for model_name in cv_results.keys():\n",
                "    print(f'now evaluating: {model_name}')\n",
                "    all_model_results[('model_name', '')] += [model_name]\n",
                "    for label_type in ['purpose', 'mode', 'replaced']:\n",
                "        # get results\n",
                "        results = get_clf_metrics(cv_results[model_name],\n",
                "                                  label_type,\n",
                "                                  keep_nopred=True,\n",
                "                                  ignore_custom=False)\n",
                "\n",
                "        if label_type == \"purpose\":\n",
                "            purpose_results[\"%s_pred\" % model_name] = results['label_pred']\n",
                "            purpose_results[\"%s_true\" % model_name] = results['label_true']\n",
                "\n",
                "        # update our dict of aggregate results\n",
                "        all_model_results[('trips without prediction', label_type)] += [\n",
                "            results['n_trips_without_prediction']\n",
                "        ]\n",
                "        all_model_results[('accuracy overall',\n",
                "                           label_type)] += [results['accuracy']]\n",
                "        all_model_results[('accuracy of trips w predictions', label_type)] += [\n",
                "            results['accuracy'] * len(results['label_true']) /\n",
                "            (len(results['label_true']) -\n",
                "             results['n_trips_without_prediction'])\n",
                "        ]\n",
                "        all_model_results[('f1 weighted',\n",
                "                           label_type)] += [results['weighted_f_score']]\n",
                "\n",
                "all_model_results_df = pd.DataFrame(all_model_results)\n",
                "all_model_results_df.to_csv('all_model_results.csv')"
            ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "purpose_results_df = pd.concat(purpose_results, axis=1, join='outer')\n",
          "purpose_results_df.to_csv(\"compare_true_pred_models.csv\")"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "purpose_results_df[['fixed-width (O-D)_pred', 'fixed-width (O-D)_true', 'fixed-width (O-D, destination)_pred', 'fixed-width (O-D, destination)_true']].head(n=20)"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "purpose_results_df[['fixed-width (O-D)_pred', 'fixed-width (O-D)_true', 'fixed-width (O-D, destination)_pred', 'fixed-width (O-D, destination)_true']].loc[:2000].sample(n=20)"
         ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_model_results_df.sort_values(by=[('accuracy overall', 'purpose')], axis=0)"
            ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "# dropping the fixed-width (O-D, destination) results since they are significantly worse than the others\n",
          "# and we don't have the time to figure out why\n",
          "all_model_results_df = all_model_results_df[all_model_results_df['model_name'] != 'fixed-width (O-D, destination)']\n",
          "all_model_results_df.to_csv('all_model_results.csv')"
         ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### visualize performance (bar graphs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(code copied from tour_model_eval/eval_comparison_plots.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### prep the dataframe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "multi_index = pd.MultiIndex.from_product([[\n",
                "    \"trips without prediction\", \"accuracy overall\",\n",
                "    \"accuracy of trips with predictions\", \"f1 weighted\"\n",
                "], [\"mode\", \"purpose\", \"replaced\"]])\n",
                "\n",
                "all_eval_results = pd.read_csv(\"all_model_results.csv\",\n",
                "                               header=[0,\n",
                "                                       1]).drop(columns=[\"Unnamed: 0_level_0\"])\n",
                "\n",
                "all_eval_results.set_index(\"model_name\", inplace=True)\n",
                "all_eval_results.head()\n",
                "all_eval_results = all_eval_results.transpose().reset_index()\n",
                "all_eval_results.rename(columns={\n",
                "    \"level_0\": \"metric\",\n",
                "    \"level_1\": \"label_type\"\n",
                "},\n",
                "                        inplace=True)\n",
                "\n",
                "\n",
                "def remove_brackets(cn):\n",
                "    if type(cn) == tuple:\n",
                "        return cn[0]\n",
                "    else:\n",
                "        return cn\n",
                "\n",
                "\n",
                "all_eval_results = all_eval_results.rename(\n",
                "    mapper=lambda cn: remove_brackets(cn), axis=1)\n",
                "\n",
                "all_eval_results.set_index(\"label_type\", inplace=True)\n",
                "\n",
                "all_eval_results = all_eval_results[\n",
                "    all_eval_results.metric != \"trips without prediction\"]\n",
                "all_eval_results = all_eval_results[\n",
                "    all_eval_results.metric != \"accuracy of trips w predictions\"]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.style.use('default')\n",
                "fig, ax_arr = plt.subplots(nrows=1,\n",
                "                           ncols=2,\n",
                "                           sharex=True,\n",
                "                           sharey=False,\n",
                "                           figsize=(12, 3))\n",
                "ax_list = ax_arr\n",
                "titles = ['Accuracy', 'Weighted F-score']\n",
                "labels = ['Purpose', 'Mode', 'Replaced Mode']\n",
                "\n",
                "for i, (metric, result_df) in enumerate(\n",
                "        all_eval_results[all_eval_results.metric != \"trips without prediction\"]\n",
                "        .groupby(\"metric\")):\n",
                "    print(f\"plotting {metric} at location {i}\")\n",
                "    result_df.plot(kind=\"bar\",\n",
                "                   ax=ax_list[i],\n",
                "                   title=titles[i],\n",
                "                   legend=False,\n",
                "                   ylim=(0, 1))\n",
                "    plt.draw()\n",
                "    ax_list[i].set_xticklabels(labels, rotation=0)\n",
                "ax_list[0].legend(loc=\"lower left\", bbox_to_anchor=(0.0, -0.5), ncol=3)\n",
                "\n",
                "ax_list[0].set_xlabel('')\n",
                "ax_list[1].set_xlabel('')\n",
                "\n",
                "# plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### evaluate performance of different models against the size of each user's dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "evaluate each algorithm's purpose prediction accuracy against the number of trips per user"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_by_user = []\n",
                "\n",
                "results_by_user = {\n",
                "    ('', 'user_id'): [],\n",
                "    ('', 'num_labeled_trips'): [],\n",
                "}\n",
                "# populate the dictionary\n",
                "for model_name in model_names:\n",
                "    results_by_user[('accuracy', model_name)] = []\n",
                "for model_name in model_names:\n",
                "    results_by_user[('f1 weighted', model_name)] = []\n",
                "\n",
                "for user in all_users:\n",
                "    results_by_user[('', 'user_id')] += [user]\n",
                "    results_by_user[('', 'num_labeled_trips')] += [\n",
                "        len(expanded_labeled_trip_df_map[user])\n",
                "    ]\n",
                "\n",
                "    for model_name in model_names:\n",
                "        cv_for_model = cv_results[model_name]\n",
                "\n",
                "        if isinstance(cv_for_model.user_id.unique()[0], UUID):\n",
                "            pass\n",
                "        else:\n",
                "            user = str(user)\n",
                "\n",
                "        if user in cv_for_model.user_id.unique() and not all(\n",
                "                cv_for_model.loc[cv_for_model.user_id == user,\n",
                "                                 'purpose_pred'].isna()):\n",
                "            results = get_clf_metrics(\n",
                "                cv_for_model.loc[cv_for_model.user_id == user],\n",
                "                label_type='purpose',\n",
                "                keep_nopred=True,\n",
                "                ignore_custom=False)\n",
                "\n",
                "            results_by_user[('accuracy', model_name)] += [results['accuracy']]\n",
                "            results_by_user[('f1 weighted',\n",
                "                             model_name)] += [results['weighted_f_score']]\n",
                "        else:\n",
                "            results_by_user[('accuracy', model_name)] += [np.nan]\n",
                "            results_by_user[('f1 weighted', model_name)] += [np.nan]\n",
                "\n",
                "size_performance_df = pd.DataFrame(results_by_user)\n",
                "size_performance_df.to_csv(\"dataset size vs performance for all algs.csv\")\n",
                "size_performance_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.style.use('default')\n",
                "\n",
                "model_names = [\n",
                "    'DBSCAN+SVM (O-D, destination)',\n",
                "    'random forests (O-D, destination clusters)',\n",
                "    'random forests (coordinates)'\n",
                "]\n",
                "\n",
                "fig, axs = plt.subplots(1, 2, figsize=(11, 4), sharex=True)\n",
                "for i in range(len(model_names)):\n",
                "    model_name = model_names[i]\n",
                "    print(model_name)\n",
                "    axs[0].scatter(\n",
                "        size_performance_df[('', 'num_labeled_trips')],\n",
                "        size_performance_df[('accuracy', model_name)],\n",
                "        s=10,\n",
                "        label=model_names[i])\n",
                "    axs[1].scatter(\n",
                "        size_performance_df[('', 'num_labeled_trips')],\n",
                "        size_performance_df[('f1 weighted', model_name)],\n",
                "        s=10,\n",
                "        label=model_names[i])\n",
                "axs[0].set_ylabel('Accuracy for Purpose')\n",
                "axs[1].set_ylabel('Weighted F-score for Purpose')\n",
                "axs[0].set_ylim(-0.1, 1.1)\n",
                "axs[1].set_ylim(-0.1, 1.1)\n",
                "\n",
                "axs[0].set_xlabel('Number of Labeled Trips')\n",
                "axs[1].set_xlabel('Number of Labeled Trips')\n",
                "\n",
                "# fig.suptitle(\n",
                "#     'Comparison of Purpose Prediction Performance Against User Dataset Size')\n",
                "axs[0].legend(loc='upper left', bbox_to_anchor=(0, -0.15), ncol=3)\n",
                "# plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "import scipy as scipy"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "def func(x, a, b, c):\n",
          "    return a * np.log(b * x) + c"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model_names = [\n",
          "    'DBSCAN+SVM (O-D, destination)',\n",
          "    'random forests (O-D, destination clusters)',\n",
          "    'random forests (coordinates)'\n",
          "]\n",
          "\n",
          "size_performance_curve_fit = size_performance_df.dropna().sort_values(by=('', 'num_labeled_trips'))\n",
          "fig, axs = plt.subplots(1, 3, figsize=(11, 4), sharex=True, sharey=True)\n",
          "for i in range(len(model_names)):\n",
          "    model_name = model_names[i]\n",
          "    print(model_name)\n",
          "    xdata = size_performance_curve_fit[('', 'num_labeled_trips')]\n",
          "    ydata = size_performance_curve_fit[('f1 weighted', model_name)]\n",
          "    print(\"xdata has the following nan %s\" % np.nonzero(np.isnan(xdata).to_numpy()))\n",
          "    print(\"ydata has the following nan %s\" % np.nonzero(np.isnan(ydata).to_numpy()))\n",
          "    popt, pcov = scipy.optimize.curve_fit(func, xdata, ydata)\n",
          "    print(\"After curve fitting, parameters are %s\" % popt)\n",
          "    axs[i].scatter(xdata, ydata, s=10)\n",
          "    axs[i].plot(xdata, func(xdata, *popt), \"r-\")\n",
          "    axs[i].set_xlabel('Number of Labeled Trips')\n",
          "    axs[i].set_title(model_names[i])\n",
          "\n",
          "axs[0].set_ylabel('Weighted F-score for Purpose')\n",
          "axs[0].set_ylim(-0.1, 1.1)\n",
          "\n",
          "# fig.suptitle(\n",
          "#     'Comparison of Purpose Prediction Performance Against User Dataset Size')\n",
          "# plt.tight_layout()\n",
          "plt.show()    "
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model_names = [\n",
          "    'DBSCAN+SVM (O-D, destination)',\n",
          "    'random forests (O-D, destination clusters)',\n",
          "    'random forests (coordinates)'\n",
          "]\n",
          "\n",
          "size_performance_curve_fit = size_performance_df.dropna().sort_values(by=('', 'num_labeled_trips'))\n",
          "fig, axs = plt.subplots(1, 1, figsize=(12, 6), sharex=True, sharey=True)\n",
          "for i in range(len(model_names)):\n",
          "    model_name = model_names[i]\n",
          "    print(model_name)\n",
          "    xdata = size_performance_curve_fit[('', 'num_labeled_trips')]\n",
          "    ydata = size_performance_curve_fit[('f1 weighted', model_name)]\n",
          "    print(\"xdata has the following nan %s\" % np.nonzero(np.isnan(xdata).to_numpy()))\n",
          "    print(\"ydata has the following nan %s\" % np.nonzero(np.isnan(ydata).to_numpy()))\n",
          "    popt, pcov = scipy.optimize.curve_fit(func, xdata, ydata)\n",
          "    # kneedle = kneed.KneeLocator(xdata, ydata, S=1.0, curve=\"concave\", direction=\"increasing\")\n",
          "    # print(kneedle.knee, kneedle.knee_y)\n",
          "    print(\"After curve fitting, parameters are %s\" % popt)\n",
          "    axs.scatter(xdata, ydata, s=10, label=model_names[i])\n",
          "    axs.plot(xdata, func(xdata, *popt), label=(\"%0.3f ln (%0.3f x) + %0.3f\" % tuple(popt)))\n",
          "    # x_knees = [125, 375]\n",
          "    # y_knees = func(np.array([x_knees]), *popt)\n",
          "    # axs.vlines(x_knees, -0.1, y_knees, linestyles=\"dashed\")\n",
          "\n",
          "axs.set_xlabel('Number of Labeled Trips')\n",
          "axs.set_ylabel('Weighted F-score for Purpose')\n",
          "axs.set_ylim(-0.1, 1.1)\n",
          "# axs.set_xticks(list(axs.get_xticks()) + x_knees)\n",
          "\n",
          "axs.legend()\n",
          "# fig.suptitle(\n",
          "#     'Comparison of Purpose Prediction Performance Against User Dataset Size')\n",
          "# plt.tight_layout()\n",
          "plt.show()    "
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "73ac5b45931ab4dd3f8e07a8d0e5daf0146eed4821bf42374f6ac6fa4af28c83"
        },
        "kernelspec": {
            "display_name": "Python 3.7.12 ('emission-private-eval')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
