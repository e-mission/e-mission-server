{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finnish-despite",
   "metadata": {},
   "source": [
    "# Experimenting with bin radius\n",
    "\n",
    "In this notebook, we are going to experiment with various values for the radius used to bin trips together. It is critical that we get bins that are large enough for appropriate modeling, without clumping together trips that are not actually related.\n",
    "\n",
    "We are going to do this by focusing on purpose labels that are likely to be repeated, such as home and work. Prior work has indicated that these are repeated for the vast majority of users. We can then use the empirical data from that to determine the radius. We may even be able to come up with a data-driven method to determine the radius for an individual phone/context.\n",
    "\n",
    "We focus on the purpose since it is most likely to have a 1:1 mapping with unique locations. Both the modes can easily be used for multiple trips whether or not they are unique.\n",
    "\n",
    "This notebook is intended to be run on the **CanBikeCO dataset, participant only, until Jan 31**. It will probably work for other datasets as well - I have tried to avoid hardcoding values - but I have not tested against them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-seventh",
   "metadata": {},
   "source": [
    "### First, we read the data and extract the most common purpose labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geojson as gj\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.element as bre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as pltc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.decorations.trip_queries as esdtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = esta.TimeSeries.get_uuid_list()\n",
    "confirmed_trip_df_map = {}\n",
    "labeled_trip_df_map = {}\n",
    "expanded_trip_df_map = {}\n",
    "for u in all_users:\n",
    "    ts = esta.TimeSeries.get_time_series(u)\n",
    "    ct_df = ts.get_data_df(\"analysis/confirmed_trip\")\n",
    "    confirmed_trip_df_map[u] = ct_df\n",
    "    labeled_trip_df_map[u] = esdtq.filter_labeled_trips(ct_df)\n",
    "    expanded_trip_df_map[u] = esdtq.expand_userinputs(labeled_trip_df_map[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-salmon",
   "metadata": {},
   "source": [
    "### Let's focus on a single user first, before expanding the analysis to all users\n",
    "\n",
    "Let's pick a user with the median number of labeled trips, so we don't get super excited about the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trips_df = pd.DataFrame([[u, len(confirmed_trip_df_map[u]), len(labeled_trip_df_map[u])] for u in all_users], columns=[\"user_id\", \"all_trips\", \"labeled_trips\"]); n_trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user = n_trips_df[n_trips_df.labeled_trips == n_trips_df.labeled_trips.median()].user_id.iloc[0]; median_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-circumstances",
   "metadata": {},
   "source": [
    "So we end up with a user with ~ 200 confirmed and labeled trips, which is not too bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-nashville",
   "metadata": {},
   "source": [
    "#### Visualizing three purposes, but not necessarily the top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user_df = expanded_trip_df_map[median_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user_df.purpose_confirm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geojson_for_purpose(exp_df, purpose):\n",
    "    purpose_trips = exp_df[exp_df.purpose_confirm == purpose]\n",
    "    # purpose_locs = gj.FeatureCollection([gj.Feature(geometry=p, properties=p) for p in list(median_user_df.head().end_loc)])\n",
    "    # return folium.features.GeoJson(purpose_locs, popup=folium.features.GeoJsonPopup(fields=[\"coordinates\"]))\n",
    "    purpose_locs = gj.FeatureCollection(purpose_trips.end_loc.to_list())\n",
    "    return folium.features.GeoJson(purpose_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-springer",
   "metadata": {},
   "source": [
    "This user does have \"home\" as the most common purpose, but \"transit transfer\" and \"personal med\" are above \"work\". Let's start with focusing on home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-neutral",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,3,1).add_child(folium.Map().add_child(get_geojson_for_purpose(median_user_df, \"home\")))\n",
    "fig.add_subplot(1,3,2).add_child(folium.Map().add_child(get_geojson_for_purpose(median_user_df, \"transit_transfer\")))\n",
    "fig.add_subplot(1,3,3).add_child(folium.Map().add_child(get_geojson_for_purpose(median_user_df, \"work\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-netherlands",
   "metadata": {},
   "source": [
    "The visualizations are interesting.\n",
    "- Home seems to have a very clear clustering with a couple of outliers.\n",
    "- Transit transfers are more spread out, although there are also some clear clusters, around Denver Union Station, for example.\n",
    "- And while work is more spread out, there are also some clear clusters visible.\n",
    "\n",
    "Let's see if we can plot this out in 2-D but not as a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_df_for_purpose(exp_df, purpose, loc_field=\"end_loc\"):\n",
    "    # Reuse the same function to get the loc_df\n",
    "    purpose_trips = exp_df\n",
    "    if purpose is not None:\n",
    "        purpose_trips = exp_df[exp_df.purpose_confirm == purpose]\n",
    "    end_loc_df = pd.DataFrame(purpose_trips[loc_field].apply(lambda p: p[\"coordinates\"]).to_list(), columns=[\"longitude\", \"latitude\"])\n",
    "    # display.display(end_loc_df.head())\n",
    "    return end_loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "get_loc_df_for_purpose(median_user_df, \"home\").plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", ax=ax)\n",
    "ax = fig.add_subplot(1,3,2)\n",
    "get_loc_df_for_purpose(median_user_df, \"transit_transfer\").plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", ax=ax)\n",
    "ax = fig.add_subplot(1,3,3)\n",
    "get_loc_df_for_purpose(median_user_df, \"transit_transfer\").plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-function",
   "metadata": {},
   "source": [
    "#### Using these to get an estimate of the radius\n",
    "\n",
    "It looks like we can use at least the \"home\" purpose to get an estimate of the radius to use. Let's calculate the distance matrix for each type of purpose separately and explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df = median_user_df[median_user_df.purpose_confirm == \"home\"]\n",
    "end_loc_df = pd.DataFrame(home_df.end_loc.apply(lambda p: p[\"coordinates\"]).to_list(), columns=[\"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics.pairwise as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(loc_df):\n",
    "    EARTH_RADIUS = 6371000\n",
    "    radians_lat_lon = np.radians(loc_df[[\"latitude\", \"longitude\"]])\n",
    "    dist_matrix_meters = pd.DataFrame(smp.haversine_distances(radians_lat_lon, radians_lat_lon) * 6371000)\n",
    "    return dist_matrix_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dist_matrix_meters = get_distance_matrix(end_loc_df)\n",
    "home_dist_series_meters = pd.Series(np.ravel(home_dist_matrix_meters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dist_series_meters.quantile([0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_loc_df[\"90%\"] = sc.DBSCAN(home_dist_series_meters.quantile(0.90), metric=\"precomputed\").fit(home_dist_matrix_meters).labels_\n",
    "end_loc_df[\"95%\"] = sc.DBSCAN(home_dist_series_meters.quantile(0.95), metric=\"precomputed\").fit(home_dist_matrix_meters).labels_\n",
    "end_loc_df[\"99%\"] = sc.DBSCAN(home_dist_series_meters.quantile(0.99), metric=\"precomputed\").fit(home_dist_matrix_meters).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_loc_df[\"90%\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.get_cmap(\"tab20\", 2).colors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colormap(labels):\n",
    "    # If we have any outliers, noise etc\n",
    "    if np.count_nonzero(labels.unique() == -1) > 0:\n",
    "        # we have some noise\n",
    "        # always put red first to make sure that outliers are red\n",
    "        return pltc.ListedColormap([\"red\"] + plt.get_cmap(\"tab20\", np.count_nonzero(labels.unique() != -1)).colors.tolist())\n",
    "    else:\n",
    "        # we don't have any noise, so no need to add red\n",
    "        return plt.get_cmap(\"tab20\", np.count_nonzero(labels.unique() != -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "# end_loc_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", color = end_loc_df[\"90%\"].apply(lambda c: colors[c]), ax=ax, colorbar=False)\n",
    "end_loc_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = \"90%\", cmap=get_colormap(end_loc_df[\"90%\"]), ax=ax, colorbar=False)\n",
    "ax = fig.add_subplot(1,3,2)\n",
    "end_loc_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = \"95%\", cmap=get_colormap(end_loc_df[\"95%\"]), ax=ax, colorbar=False)\n",
    "ax = fig.add_subplot(1,3,3)\n",
    "end_loc_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = \"99%\", cmap=get_colormap(end_loc_df[\"99%\"]), ax=ax, colorbar=False)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-applicant",
   "metadata": {},
   "source": [
    "Based on this, the 90% radius of around 100 meters looks good. Let's see if that is generalizable to the two other categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-syracuse",
   "metadata": {},
   "source": [
    "####  Generalizing to other purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_loc_clusters(loc_df, quantiles, radii, min_samples=5):\n",
    "    print(\"in add_loc_clusters %d\" % len(loc_df))\n",
    "    dist_matrix_meters = get_distance_matrix(loc_df)\n",
    "    dist_series_meters = pd.Series(np.ravel(dist_matrix_meters))\n",
    "    if radii is None:\n",
    "        assert quantiles is not None\n",
    "        radii = dist_series_meters.quantile(quantiles)\n",
    "    for r in radii:\n",
    "        loc_df[f\"{r}_m\"] = pd.Categorical(sc.DBSCAN(r, metric=\"precomputed\", min_samples=min_samples).fit(dist_matrix_meters).labels_)\n",
    "    # Map to categorical variables so the plotting is easier\n",
    "    # for r in radii:\n",
    "    #    loc_df[str(r)] = loc_df[str(r)].apply(lambda l: \"c%d\"%l if l !=-1 else \"N\")\n",
    "    return radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,10))\n",
    "quantiles = [0.9, 0.95, 0.99]\n",
    "fig_index = 0\n",
    "for p in [\"home\", \"transit_transfer\", \"work\"]: \n",
    "    p_loc_df = get_loc_df_for_purpose(median_user_df, p)\n",
    "    radii = add_loc_clusters(p_loc_df, quantiles, None)\n",
    "    # print(radii)\n",
    "    for r in radii:\n",
    "        fig_index = fig_index + 1\n",
    "        ax = fig.add_subplot(3,3,fig_index)\n",
    "        p_loc_df.plot(title=\"Radius = %.2f meters\" % r, kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = f\"{r}_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-twist",
   "metadata": {},
   "source": [
    "This is interesting as well, but the median values for the other purposes are so high that the evaluated clusters are too big. For example, for the work purpose, all the trips are in the same cluster. We want the clusters to look like one dot in the visualization, even at the 0.9 quantile. Let's hardcode the evaluation radii and retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,10))\n",
    "radii = [100, 300, 500]\n",
    "fig_index = 0\n",
    "for p in [\"home\", \"transit_transfer\", \"work\"]:\n",
    "    p_loc_df = get_loc_df_for_purpose(median_user_df, p)\n",
    "    radii = add_loc_clusters(p_loc_df, None, radii)\n",
    "    # print(radii)\n",
    "    for r in radii:\n",
    "        fig_index = fig_index + 1\n",
    "        ax = fig.add_subplot(3,3,fig_index)\n",
    "        p_loc_df.plot(title=\"Radius = %.2f meters\" % r, kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = f\"{r}_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-township",
   "metadata": {},
   "source": [
    "There is a max of three clusters above. All the green points are outliers. The other color points are actual clusters. 100 seems to be too tight - for \"transit transfer\" above, we can see that there are green points really close to both label 0 and label 1 that look like they are overlapping, but that are not in the same cluster. There doesn't seem to be a huge difference between 300 and 500, at least for these examples.\n",
    "\n",
    "Note that because DBSCAN takes the biggest between between **any two points** as the parameter, we can have some linear clusters (as for work) as opposed to circular clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-belief",
   "metadata": {},
   "source": [
    "#### Generalize to all purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user_df.purpose_confirm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user_df.purpose_confirm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,40))\n",
    "radii = [300, 500, 700]\n",
    "fig_index = 0\n",
    "purpose_list = pd.Series(median_user_df.purpose_confirm.unique()).dropna()\n",
    "for i, p in enumerate(purpose_list):\n",
    "    p_loc_df = get_loc_df_for_purpose(median_user_df, p)\n",
    "    radii = add_loc_clusters(p_loc_df, quantiles=None, radii=radii)\n",
    "    # print(radii)\n",
    "    for r in radii:\n",
    "        fig_index = fig_index + 1\n",
    "        ax = fig.add_subplot(len(purpose_list), 3,fig_index)\n",
    "        p_loc_df.plot(title=\"%s: Radius = %.2f meters\" % (p, r), kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = f\"{r}_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-twelve",
   "metadata": {},
   "source": [
    "The differences are slight, but based on this, 500 seems to be the best option overall.\n",
    "- It is better than 300 in shopping\n",
    "- It is better than 700 in work and exercise\n",
    "\n",
    "I would be OK with 300 because it is fairly close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-folder",
   "metadata": {},
   "source": [
    "### Summarize and generalize for all users\n",
    "\n",
    "Now, let's summarize and generalize for all users. The summary should make it easier for us to identify differences and look at them more carefully. We use the number of points in clusters and the number of outliers as our summary metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_summary(user_df, radii, min_samples=5):\n",
    "    cluster_summary = []\n",
    "    curr_user_id = user_df.user_id.iloc[0]\n",
    "    print(f\"About to get loc_df for user_df of length {len(user_df)}\")\n",
    "    purpose_list = pd.Series(user_df.purpose_confirm.unique()).dropna()\n",
    "    for p in purpose_list:\n",
    "        print(f\"For {curr_user_id}, in get_cluster_summary, getting loc_df for {p}\")\n",
    "        p_loc_df = get_loc_df_for_purpose(user_df, p)\n",
    "        radii = add_loc_clusters(p_loc_df, None, radii, min_samples)\n",
    "        curr_entry = {\"user_id\": curr_user_id, \"purpose\": p, \"n_locs\": len(p_loc_df)}\n",
    "        for r in radii:\n",
    "            curr_col = p_loc_df[f\"{r}_m\"]\n",
    "            # print(curr_col)\n",
    "            curr_entry.update({\"n_clusters_%s\" % r: np.count_nonzero(curr_col.unique() != -1),\n",
    "                                \"n_valid_%s\" % r: np.count_nonzero(curr_col != -1),\n",
    "                                \"n_noise_%s\" % r: np.count_nonzero(curr_col == -1)})\n",
    "        cluster_summary.append(curr_entry)\n",
    "    return pd.DataFrame(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is sufficiently different that we create a new function for it\n",
    "# TODO: See if it is possible to unify both these functions\n",
    "def get_cluster_summary_no_grouping(user_df, radii, min_samples=5):\n",
    "    cluster_summary = []\n",
    "    # We need to reset the index here because the user_df is actually filtered from a larger dataset\n",
    "    # confirmed trip -> labeled trip\n",
    "    # so there can be gaps in the index and can have entries beyond the length of the dataframe\n",
    "    # while creating get_loc_df_for_purpose, we create a new dataframe from the end_loc\n",
    "    # so this index has no gaps and is limited to the length of the dataframe\n",
    "    # so when we try to select the loc entries based on the user_df index, we can fail\n",
    "    # resetting the index allows us to ensure that the indices match up\n",
    "    # not sure if we need to copy the dataframe first (changes to the view don't propagate backwards anyway)\n",
    "    # but better to be safe than sorry\n",
    "    user_df = user_df.copy()\n",
    "    user_df.reset_index(inplace=True)\n",
    "    curr_user_id = user_df.user_id.iloc[0]\n",
    "    print(f\"About to get loc_df for user_df {curr_user_id} of length {len(user_df)}\")\n",
    "    all_loc_df = get_loc_df_for_purpose(user_df, purpose=None)\n",
    "    print(f\"all_loc_df length {len(user_df)}\")\n",
    "    radii = add_loc_clusters(all_loc_df, None, radii, min_samples)\n",
    "    purpose_list = pd.Series(user_df.purpose_confirm.unique()).dropna()\n",
    "    for p in purpose_list:\n",
    "        # print(f\"user_df {len(user_df)}, all_loc_df {len(all_loc_df)}\")\n",
    "        # print(f\"Going to select index {user_df[user_df.purpose_confirm == p].index} of length {len(user_df[user_df.purpose_confirm == p])} from {all_loc_df.index}\")\n",
    "        # print(f\"Before filtering, index {user_df.index}\")\n",
    "        p_loc_df = all_loc_df.loc[user_df[user_df.purpose_confirm == p].index]\n",
    "        # print(f\"For {curr_user_id}, in get_cluster_summary, getting loc_df for {p}\")\n",
    "        curr_entry = {\"user_id\": curr_user_id, \"purpose\": p, \"n_locs\": len(p_loc_df)}\n",
    "        for r in radii:\n",
    "            curr_col = p_loc_df[f\"{r}_m\"]\n",
    "            # print(curr_col)\n",
    "            curr_entry.update({\"n_clusters_%s\" % r: np.count_nonzero(curr_col.unique() != -1),\n",
    "                              \"n_valid_%s\" % r: np.count_nonzero(curr_col != -1),\n",
    "                               \"n_noise_%s\" % r: np.count_nonzero(curr_col == -1)})\n",
    "        cluster_summary.append(curr_entry)\n",
    "    print(f\"Returning dataframe of length {len(cluster_summary)}, matching purpose length {len(purpose_list)}\")\n",
    "    return pd.DataFrame(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user_cluster_summary = get_cluster_summary(median_user_df, radii = [300, 500, 700])\n",
    "median_user_cluster_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_median_user = median_user_cluster_summary.query(\"(n_valid_500 - n_valid_300) > 1 or (n_valid_700 - n_valid_500) > 1\")\n",
    "diff_median_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,10))\n",
    "radii = [300, 500, 700]\n",
    "fig_index = 0\n",
    "purpose_list = pd.Series(diff_median_user.purpose.unique()).dropna()\n",
    "for i, p in enumerate(purpose_list):\n",
    "    p_loc_df = get_loc_df_for_purpose(median_user_df, p)\n",
    "    radii = add_loc_clusters(p_loc_df, None, radii)\n",
    "    # print(radii)\n",
    "    for r in radii:\n",
    "        fig_index = fig_index + 1\n",
    "        ax = fig.add_subplot(len(purpose_list), 3,fig_index)\n",
    "        p_loc_df.plot(title=\"%s: Radius = %.2f meters\" % (p, r), kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = f\"{r}_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-pennsylvania",
   "metadata": {},
   "source": [
    "### Now for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_diffs = []\n",
    "for u in all_users:\n",
    "    curr_user_df = expanded_trip_df_map[u]\n",
    "    if len(curr_user_df) == 0:\n",
    "        continue\n",
    "    # print(\"Continuing with %s\" % (len(curr_user_df)))\n",
    "    curr_cluster_summary = get_cluster_summary(curr_user_df, [300,500,700])\n",
    "    curr_user_diff = curr_cluster_summary.query(\"(n_valid_500 - n_valid_300) > 1 or (n_valid_700 - n_valid_500) > 1\")\n",
    "    all_user_diffs.append(curr_user_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_diffs_df = pd.concat(all_user_diffs)\n",
    "all_user_diffs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-marketing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# unique_users_with_diffs = pd.Series(all_user_diffs.user_id)\n",
    "ax = all_user_diffs_df[[\"n_valid_300\", \"n_valid_500\", \"n_valid_700\", \"n_locs\"]].plot(kind=\"bar\", figsize=(20,10))\n",
    "ax.set_xticklabels(all_user_diffs_df.user_id.apply(lambda u: str(u)[:2]+\"_\") + all_user_diffs_df.purpose)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-better",
   "metadata": {},
   "source": [
    "##### With separate axes so we can see the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_diff_df = all_user_diffs_df.query(\"(n_valid_500 - n_valid_300) > 5 or (n_valid_700 - n_valid_500) > 5\")\n",
    "fig = plt.Figure(figsize=(20,50))\n",
    "radii = [300, 500, 700]\n",
    "fig_index = 0\n",
    "user_purpose_list = high_diff_df[[\"user_id\", \"purpose\"]]\n",
    "for i, row in user_purpose_list.iterrows():\n",
    "    # print(row)\n",
    "    p_loc_df = get_loc_df_for_purpose(expanded_trip_df_map[row.user_id], row.purpose)\n",
    "    radii = add_loc_clusters(p_loc_df, None, radii)\n",
    "    # print(radii)\n",
    "    for r in radii:\n",
    "        fig_index = fig_index + 1\n",
    "        ax = fig.add_subplot(len(high_diff_df), 3,fig_index)\n",
    "        p_loc_df.plot(title=\"%s: Radius = %.2f meters\" % (p, r), kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = f\"{r}_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-firmware",
   "metadata": {},
   "source": [
    "##### With shared x and y, so we can compare them better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_diff_df = all_user_diffs_df.query(\"(n_valid_500 - n_valid_300) > 5 or (n_valid_700 - n_valid_500) > 5\")\n",
    "fig = plt.Figure(figsize=(20,50))\n",
    "axarr = fig.subplots(len(high_diff_df), 3)\n",
    "radii = [300, 500, 700]\n",
    "fig_index = 0\n",
    "user_purpose_list = high_diff_df[[\"user_id\", \"purpose\"]]\n",
    "for (i, row), carr in zip(user_purpose_list.iterrows(),axarr):\n",
    "    # print(row)\n",
    "    p_loc_df = get_loc_df_for_purpose(expanded_trip_df_map[row.user_id], row.purpose)\n",
    "    radii = add_loc_clusters(p_loc_df, None, radii)\n",
    "    # print(radii)\n",
    "    for r, ax in zip(radii, carr):\n",
    "        fig_index = fig_index + 1\n",
    "        # ax = fig.add_subplot(len(high_diff_df), 3,fig_index, sharex=figax[0], sharey=figax[0])\n",
    "        p_loc_df.plot(title=\"%s: Radius = %.2f meters\" % (p, r), kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = f\"{r}_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-delight",
   "metadata": {},
   "source": [
    "This is hard to work with. The 700 meters is clearly better than 500 in terms of grouping, but that is only to be expected. Is that going to be too large? Let's **visualize on a map to confirm**.\n",
    "\n",
    "Picking row 1, which seems to have quite a few changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_disp_entry = user_purpose_list.iloc[1]\n",
    "fig = bre.Figure()\n",
    "fig.add_child(folium.Map().add_child(get_geojson_for_purpose(expanded_trip_df_map[to_disp_entry.user_id], to_disp_entry.purpose)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-azerbaijan",
   "metadata": {},
   "source": [
    "Looking at three groups that changed between 500 and 700 meters:\n",
    "\n",
    "#### cluster 4 from 700m (brown, top left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-assignment",
   "metadata": {},
   "source": [
    "#### cluster 8 from 700m (light blue, middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-capture",
   "metadata": {},
   "source": [
    "#### cluster 7 from 700m (yellow, bottom right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-freeze",
   "metadata": {},
   "source": [
    "In all of these, the blue oval indicates the cluster at 300m, while the red oval indicates the cluster at 700m.\n",
    "Based on these, we should almost certainly not go to 700m - in all the maps here, the red ovals are way too large.\n",
    "However, I am surprised that some of the points are not clustered. Concretely, the two points in the bottom right of the map seem like they should be clustered. There are similar clusters in other locations as well.\n",
    "\n",
    "Let's look at the actual distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_disp_entry = user_purpose_list.iloc[1]\n",
    "radii = [500,700]\n",
    "p_loc_df = get_loc_df_for_purpose(expanded_trip_df_map[to_disp_entry.user_id], to_disp_entry.purpose)\n",
    "radii = add_loc_clusters(p_loc_df, None, radii)\n",
    "p_loc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-postcard",
   "metadata": {},
   "source": [
    "#### checking distances: cluster 4 from 700m (brown, top left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_disp_distance_matrix = get_distance_matrix(p_loc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4_700_m = p_loc_df[p_loc_df[\"700_m\"] == 4]\n",
    "matching_500_m = p_loc_df.loc[cluster_4_700_m.index, \"500_m\"]\n",
    "matching_500_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "unclustered_500_m = matching_500_m[matching_500_m == -1]\n",
    "to_disp_distance_matrix.loc[unclustered_500_m.index, unclustered_500_m.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-pulse",
   "metadata": {},
   "source": [
    "Although the third distance > 300m, it is still < 500m, and the first two points are pretty close. Why didn't we get another cluster with just points 20 and 81. Why didn't point 93 get into the cluster? I guess the reason could be that the core sample was far enough away from it.\n",
    "\n",
    "Let's get the distance matrix for all the points in the 700m cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_disp_distance_matrix.loc[cluster_4_700_m.index, cluster_4_700_m.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-forum",
   "metadata": {},
   "source": [
    "Based on that, the core sample is probably one of 87, 99, 101, 119 or 128, since those are > 500m from points that are not in the cluser.\n",
    "\n",
    "Can we get the core samples from the DBSCAN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel = sc.DBSCAN(500, metric=\"precomputed\").fit(to_disp_distance_matrix.loc[cluster_4_700_m.index, cluster_4_700_m.index])\n",
    "print(testmodel.core_sample_indices_)\n",
    "to_disp_distance_matrix.loc[cluster_4_700_m.index, cluster_4_700_m.index].iloc[testmodel.core_sample_indices_.tolist(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel = sc.DBSCAN(500, metric=\"precomputed\", min_samples=2).fit(to_disp_distance_matrix.loc[cluster_4_700_m.index, cluster_4_700_m.index])\n",
    "print(testmodel.core_sample_indices_)\n",
    "to_disp_distance_matrix.loc[cluster_4_700_m.index, cluster_4_700_m.index].iloc[testmodel.core_sample_indices_.tolist(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_500m_clusters(c700m):\n",
    "    cluster_700_m = p_loc_df[p_loc_df[\"700_m\"] == c700m]\n",
    "    return p_loc_df.loc[cluster_700_m.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_changed_clusters_700_m = [4,7,8]\n",
    "get_500m_clusters(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(10,3))\n",
    "axarr = fig.subplots(1,3)\n",
    "for cm700m, ax in zip(saved_changed_clusters_700_m, axarr):\n",
    "    get_500m_clusters(cm700m).plot.scatter(x=\"longitude\", y=\"latitude\", c=\"500_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-welsh",
   "metadata": {},
   "source": [
    "### Important findings about DBSCAN\n",
    "\n",
    "- we can get the list of core samples from the model (we can reuse this as the cluster centroid)\n",
    "- ah!! the default parameters for DBSCAN have min_samples = 5, so we won't create clusters of size 2. Let's experiment with other values for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-archive",
   "metadata": {},
   "source": [
    "### Now for all users with min_samples = 2 (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_diffs = []\n",
    "for u in all_users:\n",
    "    curr_user_df = expanded_trip_df_map[u]\n",
    "    if len(curr_user_df) == 0:\n",
    "        continue\n",
    "    # print(\"Continuing with %s\" % (len(curr_user_df)))\n",
    "    curr_cluster_summary = get_cluster_summary(curr_user_df, [300,500,700], min_samples=2)\n",
    "    curr_user_diff = curr_cluster_summary.query(\"(n_valid_500 - n_valid_300) > 1 or (n_valid_700 - n_valid_500) > 1\")\n",
    "    all_user_diffs.append(curr_user_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_diffs_df = pd.concat(all_user_diffs)\n",
    "all_user_diffs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_users_with_diffs = pd.Series(all_user_diffs.user_id)\n",
    "ax = all_user_diffs_df[[\"n_valid_300\", \"n_valid_500\", \"n_valid_700\", \"n_locs\"]].plot(kind=\"bar\", figsize=(20,10))\n",
    "ax.set_xticklabels(all_user_diffs_df.user_id.apply(lambda u: str(u)[:2]+\"_\") + all_user_diffs_df.purpose)\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have so many clusters now that tab20 isn't going to cut it any more. Let's try something like Pastel1 instead\n",
    "def get_colormap(labels):\n",
    "    # If we have any outliers, noise etc\n",
    "    if np.count_nonzero(labels.unique() == -1) > 0:\n",
    "        # we have some noise\n",
    "        # always put red first to make sure that outliers are red\n",
    "        return pltc.ListedColormap([\"red\"] + plt.get_cmap(\"Pastel1\", np.count_nonzero(labels.unique() != -1)).colors.tolist())\n",
    "    else:\n",
    "        # we don't have any noise, so no need to add red\n",
    "        return plt.get_cmap(\"Pastel1\", np.count_nonzero(labels.unique() != -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_diff_df = all_user_diffs_df.query(\"(n_valid_500 - n_valid_300) > 5 or (n_valid_700 - n_valid_500) > 5\")\n",
    "fig = plt.Figure(figsize=(20,10))\n",
    "radii = [300, 500, 700]\n",
    "fig_index = 0\n",
    "user_purpose_list = high_diff_df[[\"user_id\", \"purpose\"]]\n",
    "for i, row in user_purpose_list.iterrows():\n",
    "    # print(row)\n",
    "    p_loc_df = get_loc_df_for_purpose(expanded_trip_df_map[row.user_id], row.purpose)\n",
    "    radii = add_loc_clusters(p_loc_df, None, radii, min_samples=2)\n",
    "    # print(radii)\n",
    "    for r in radii:\n",
    "        fig_index = fig_index + 1\n",
    "        ax = fig.add_subplot(len(high_diff_df), 3,fig_index)\n",
    "        p_loc_df.plot(title=\"%s: Radius = %.2f meters\" % (p, r), kind=\"scatter\", x=\"longitude\", y=\"latitude\", c = f\"{r}_m\", cmap=get_colormap(p_loc_df[f\"{r}_m\"]), ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-perception",
   "metadata": {},
   "source": [
    "Checking the same points as before, we now see that way more points are in clusters. There are only two entries for which there is a large difference in the number of valid entries. Fortunately, our test case is still one of them. It's hard to see the clusters now because this categorical list doesn't have nice labels. But we can clearly see that there are now clusters in all the parts where there were outliers before.\n",
    "\n",
    "- old cluster 4 from 700m:\n",
    "  - 500m: the point (pink) and the line next to it (green), both of these seem to be reasonable from the map\n",
    "  - 700m: all the points are in one cluster, which seems too large\n",
    "- old cluster 7 from 700m:\n",
    "  - 500m: two separate clusters, both of which seem reasonable from the map\n",
    "  - 700m: they are merged, which seems bad\n",
    "- old cluster 8 from 700m:\n",
    "  - 500m: the obvious cluster is a cluster; the point below the cluster is not part of it\n",
    "  - 700m: the point below is part of it, which seems bad\n",
    "\n",
    "\n",
    "- The % of valid trips has shot up significantly\n",
    "- There are a lot more entries with where there is a difference of 1 valid trip\n",
    "- There are a lot fewer entries where there is a difference > 5 valid trips; only 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-cookbook",
   "metadata": {},
   "source": [
    "# Final result of tuning: 500m radius\n",
    "\n",
    "Also, if we choose to use DBSCAN, the parameters need to be:\n",
    "- distance matrix (metric=precomputed)\n",
    "- min_samples=2\n",
    "\n",
    "Before we end, let's generate some quick graphs on the comparision between min_samples and the overall dataset quality for the selected values. If we want to plot the entire dataset, and not just the diff values, we probably want to use a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just get the cluster summary for all users and bung it into a giant dataframe\n",
    "all_user_summaries = []\n",
    "for u in all_users:\n",
    "    curr_user_df = expanded_trip_df_map[u]\n",
    "    if len(curr_user_df) == 0:\n",
    "        continue\n",
    "    # print(\"Continuing with %s\" % (len(curr_user_df)))\n",
    "    # dataframe mapping user id and purpose to n locs, n clusters, etc\n",
    "    curr_cluster_summary = get_cluster_summary(curr_user_df, [300,500,700], min_samples=2)\n",
    "    curr_cluster_summary[\"min_samples\"] = 2\n",
    "    # curr_cluster_summary = get_cluster_summary(curr_user_df, [300,500,700], min_samples=5)\n",
    "    # curr_cluster_summary[\"min_samples\"] = 5\n",
    "    all_user_summaries.append(curr_cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_user_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_summary_df = pd.concat(all_user_summaries)\n",
    "all_user_summary_df.reset_index(inplace=True)\n",
    "all_user_summary_df[\"user_id_trunc\"] = all_user_summary_df.user_id.apply(lambda u: str(u)[:2])\n",
    "all_user_summary_df[\"pct_valid\"] = all_user_summary_df.n_valid_500 / all_user_summary_df.n_locs\n",
    "all_user_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-mountain",
   "metadata": {},
   "source": [
    "### Validity per user and purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = dict(zip(all_user_summary_df[\"user_id_trunc\"].unique(), plt.get_cmap(\"tab20\", len(all_user_summary_df.user_id.unique())).colors))\n",
    "# print(colors)\n",
    "ax = all_user_summary_df.plot(kind=\"scatter\", x=\"purpose\", y=\"pct_valid\", color = all_user_summary_df[\"user_id_trunc\"].apply(lambda c: colors[c]), figsize=(20,6))\n",
    "ax.set_xticklabels(range(len(all_user_summary_df)))\n",
    "# ax.legend(colors.keys())\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_summary_df.pct_valid.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zero_df = all_user_summary_df[all_user_summary_df.pct_valid == 0]\n",
    "all_zero_df[[\"user_id\", \"purpose\", \"n_locs\", \"n_clusters_500\", \"n_valid_500\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-fifteen",
   "metadata": {},
   "source": [
    "### Validity by user only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_user_summary_df = all_user_summary_df[[\"user_id\", \"n_locs\", \"n_valid_500\"]].groupby(\"user_id\").agg('sum')\n",
    "agg_user_summary_df[\"pct_valid\"] = agg_user_summary_df.n_valid_500 / agg_user_summary_df.n_locs\n",
    "agg_user_summary_df.pct_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_user_summary_df.n_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_user_summary_df[\"user_id_trunc\"] = [str(u)[:2] for u in agg_user_summary_df.index]\n",
    "ax = agg_user_summary_df.pct_valid.plot(kind=\"bar\")\n",
    "ax.set_xticklabels(agg_user_summary_df.user_id_trunc.to_list())\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-scheme",
   "metadata": {},
   "source": [
    "This means that, even with the current, somewhat wonky dataset, more than half of the trip ends can fit into some cluster, which means that if a new trip comes in, we should be able to match at least for the purpose. Of course, for the mode, etc, we really need both ends to match, but that is a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-muscle",
   "metadata": {},
   "source": [
    "# Checking final results without separating by purpose\n",
    "\n",
    "So far, we have been grouping by the purpose before clustering. We can still do this for model building, but we won't necessarily be able to do this for model prediction because we won't (by definition) have a purpose then. Does binning by the purpose matter? Will we get dramatically different result if we don't bin by purpose first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just get the cluster summary for all users and bung it into a giant dataframe\n",
    "all_user_summaries = []\n",
    "for u in all_users:\n",
    "    curr_user_df = expanded_trip_df_map[u]\n",
    "    if len(curr_user_df) == 0:\n",
    "        continue\n",
    "    # print(\"Continuing with %s\" % (len(curr_user_df)))\n",
    "    # dataframe mapping user id and purpose to n locs, n clusters, etc\n",
    "    # display.display(curr_user_df.head())\n",
    "    curr_cluster_summary = get_cluster_summary_no_grouping(curr_user_df, [300,500,700], min_samples=2)\n",
    "    curr_cluster_summary[\"min_samples\"] = 2\n",
    "    # curr_cluster_summary = get_cluster_summary(curr_user_df, [300,500,700], min_samples=5)\n",
    "    # curr_cluster_summary[\"min_samples\"] = 5\n",
    "    all_user_summaries.append(curr_cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_summary_df = pd.concat(all_user_summaries)\n",
    "all_user_summary_df.reset_index(inplace=True)\n",
    "all_user_summary_df[\"user_id_trunc\"] = all_user_summary_df.user_id.apply(lambda u: str(u)[:2])\n",
    "all_user_summary_df[\"pct_valid\"] = all_user_summary_df.n_valid_500 / all_user_summary_df.n_locs\n",
    "all_user_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = dict(zip(all_user_summary_df[\"user_id_trunc\"].unique(), plt.get_cmap(\"tab20\", len(all_user_summary_df.user_id.unique())).colors))\n",
    "# print(colors)\n",
    "ax = all_user_summary_df.plot(kind=\"scatter\", x=\"purpose\", y=\"pct_valid\", color = all_user_summary_df[\"user_id_trunc\"].apply(lambda c: colors[c]), figsize=(20,6))\n",
    "ax.set_xticklabels(range(len(all_user_summary_df)))\n",
    "# ax.legend(colors.keys())\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_user_summary_df = all_user_summary_df.groupby(\"user_id\").agg('sum')\n",
    "agg_user_summary_df[\"pct_valid\"] = agg_user_summary_df.n_valid_500 / agg_user_summary_df.n_locs\n",
    "agg_user_summary_df.pct_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_user_summary_df[\"user_id_trunc\"] = [str(u)[:2] for u in agg_user_summary_df.index]\n",
    "ax = agg_user_summary_df.pct_valid.plot(kind=\"bar\")\n",
    "ax.set_xticklabels(agg_user_summary_df.user_id_trunc.to_list())\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-demonstration",
   "metadata": {},
   "source": [
    "Aha! Clustering without separating by purpose actually gives significantly better results. Almost everybody has almost all their trips in clusters.\n",
    "\n",
    "There are ~ 6 (user-cluster pairs) with no mappings. This result indicates that there are multiple labels in each cluster. So when we first group by the purpose, there are end locations that don't have any nearby end locations. But when we first cluster, we do have nearby end locations, just labeled with a different purpose. This can either be a genuine problem, or an issue with incorrect labels in the dataset.\n",
    "\n",
    "Note also that the total points (`n_loc`) seems to be different in the log statements and in the sum of the locations counted - e.g. for `e7b24d99-324d-4d6d-b247-9edc87d3c848`, the printed length is 81 but there are only 77 entries when we sum up the dataframe (both ways!)\n",
    "\n",
    "```\n",
    "About to get loc_df for user_df 576e37c7-ab7e-4c03-add7-02486bc3f42e of length 226\n",
    "About to get loc_df for user_df 8b563348-52b3-4e3e-b046-a0aaf4fcea15 of length 1\n",
    "About to get loc_df for user_df 5079bb93-c9cf-46d7-a643-dfc86bb05605 of length 212\n",
    "About to get loc_df for user_df feabfccd-dd6c-4e8e-8517-9d7177042483 of length 217\n",
    "About to get loc_df for user_df 113aef67-400e-4e21-a29f-d04e50fc42ea of length 33\n",
    "About to get loc_df for user_df c8b9fe22-86f8-449a-b64f-c18a8d20eefc of length 163\n",
    "About to get loc_df for user_df e7b24d99-324d-4d6d-b247-9edc87d3c848 of length 81\n",
    "About to get loc_df for user_df 1044195f-af9e-43d4-9407-60594e5e9938 of length 380\n",
    "About to get loc_df for user_df 898b1a5e-cdd4-4a0c-90e4-942fa298e456 of length 264\n",
    "About to get loc_df for user_df 1d292b85-c549-409a-a10d-746e957582a0 of length 208\n",
    "About to get loc_df for user_df cb3222a7-1e72-4a92-8b7b-2c4795402497 of length 22\n",
    "About to get loc_df for user_df 960835ac-9d8a-421d-8b8a-bf816f8a4b92 of length 618\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_user_summary_df.n_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_user_summary_df[all_user_summary_df.user_id == agg_user_summary_df.index[-2]][[\"purpose\", \"n_locs\"]].n_locs.sum(),\n",
    "expanded_trip_df_map[agg_user_summary_df.index[-2]].groupby(by=\"purpose_confirm\").agg('count').source.sum(),\n",
    "len(expanded_trip_df_map[agg_user_summary_df.index[-2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-bracket",
   "metadata": {},
   "source": [
    "Aha! There are 81 entries in total, but grouping by purpose and summing up gets us to 77. What is the list of purpose entries for this user? Can that give us a clue? If that doesn't work, we can also create two dataframes and generate a set diff to quickly determine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_trip_df_map[agg_user_summary_df.index[-2]].purpose_confirm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_trip_df_map[agg_user_summary_df.index[-2]].purpose_confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_trip_df_map[agg_user_summary_df.index[-2]].purpose_confirm.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-medicine",
   "metadata": {},
   "source": [
    "OK! So this is in fact because of nan entries for the purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-nepal",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "\n",
    "- let's run this same code against a couple of other datasets, including the staging enviroment.\n",
    "- We also need to see how clustering based on both the start and end locations will work\n",
    "\n",
    "Will probably do that in a separate notebook, generated by simplifying the code in here, and potentially pulling out into separate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-platform",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
