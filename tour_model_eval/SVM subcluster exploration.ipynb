{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from uuid import UUID\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# hack because jupyter notebook doesn't work properly through my vscode for\n",
    "# some reason and therefore cant import stuff from emission? remove this before\n",
    "# pushing\n",
    "###\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/hlu2/Documents/GitHub/e-mission-server/')\n",
    "###\n",
    "\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.decorations.trip_queries as esdtq\n",
    "import emission.core.get_database as edb\n",
    "import mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the same outputs I described, put in the unique tokens for these users\n",
    "email0 = \"replace this\"  # shankari\n",
    "email1 = \"replace this\"  # tom\n",
    "user0 = list(edb.get_uuid_db().find({\"user_email\": email0}))[0]['uuid']\n",
    "user1 = list(edb.get_uuid_db().find({\"user_email\": email1}))[0]['uuid']\n",
    "user2 = UUID('replace this')  # hannah\n",
    "\n",
    "all_users = esta.TimeSeries.get_uuid_list()\n",
    "user_list = np.append([user0, user1, user2],\n",
    "                      np.random.choice(all_users, size=10, replace=False))\n",
    "confirmed_trip_df_map = {}\n",
    "labeled_trip_df_map = {}\n",
    "expanded_labeled_trip_df_map = {}\n",
    "expanded_all_trip_df_map = {}\n",
    "for i in range(len(user_list)):\n",
    "    u = user_list[i]\n",
    "    print(u)\n",
    "    ts = esta.TimeSeries.get_time_series(u)\n",
    "    ct_df = ts.get_data_df(\"analysis/confirmed_trip\")\n",
    "\n",
    "    confirmed_trip_df_map[i] = ct_df\n",
    "    labeled_trip_df_map[i] = esdtq.filter_labeled_trips(ct_df)\n",
    "    expanded_labeled_trip_df_map[i] = esdtq.expand_userinputs(\n",
    "        labeled_trip_df_map[i])\n",
    "    expanded_all_trip_df_map[i] = esdtq.expand_userinputs(\n",
    "        confirmed_trip_df_map[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM exploration for single clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up data so that we can look at some specific clusters first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_wrangling\n",
    "from clustering import *\n",
    "\n",
    "\n",
    "def setup(user_df,\n",
    "          alg,\n",
    "          loc_type,\n",
    "          radii=[50, 100, 150, 200],\n",
    "          cluster_unlabeled=False):\n",
    "    \"\"\" copied and modified from plot_clusters() in mapping \"\"\"\n",
    "    # clean up the dataframe by dropping entries with NaN locations and\n",
    "    # resetting index because oursim needs the position of each trip to match\n",
    "    # its nominal index\n",
    "    all_trips_df = user_df.dropna(subset=['start_loc', 'end_loc']).reset_index(\n",
    "        drop=True)\n",
    "\n",
    "    # expand the 'start/end_loc' column into 'start/end_lat/lon' columns\n",
    "    all_trips_df = data_wrangling.expand_coords(all_trips_df)\n",
    "\n",
    "    labeled_trips_df = all_trips_df.loc[all_trips_df.user_input != {}]\n",
    "    df_for_cluster = all_trips_df if cluster_unlabeled else labeled_trips_df\n",
    "\n",
    "    df_for_cluster = add_loc_clusters(df_for_cluster,\n",
    "                                      radii=radii,\n",
    "                                      alg=alg,\n",
    "                                      loc_type=loc_type,\n",
    "                                      min_samples=2)\n",
    "\n",
    "    return df_for_cluster\n",
    "\n",
    "\n",
    "shankari = expanded_all_trip_df_map[0]\n",
    "tom = expanded_all_trip_df_map[1]\n",
    "hannah = expanded_all_trip_df_map[2]\n",
    "\n",
    "shankari = setup(shankari,\n",
    "                 alg='DBSCAN',\n",
    "                 loc_type='end',\n",
    "                 radii=[100, 150, 200, 250],\n",
    "                 cluster_unlabeled=False)\n",
    "tom = setup(tom,\n",
    "            alg='DBSCAN',\n",
    "            loc_type='end',\n",
    "            radii=[100, 150, 200, 250],\n",
    "            cluster_unlabeled=False)\n",
    "hannah = setup(hannah,\n",
    "               alg='DBSCAN',\n",
    "               loc_type='end',\n",
    "               radii=[100, 150, 200, 250],\n",
    "               cluster_unlabeled=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'DBSCAN'\n",
    "loc_type = 'end'\n",
    "\n",
    "c1 = hannah.loc[(hannah[f'{loc_type}_{alg}_clusters_150_m'] == 1\n",
    "                 )].loc[:, ['end_lat', 'end_lon', 'purpose_confirm']].dropna()\n",
    "\n",
    "c2 = shankari.loc[(shankari[f'{loc_type}_{alg}_clusters_200_m'] == 1\n",
    "                   )].loc[:,\n",
    "                          ['end_lat', 'end_lon', 'purpose_confirm']].dropna()\n",
    "c3 = shankari.loc[(shankari[f'{loc_type}_{alg}_clusters_150_m'] == 1\n",
    "                   )].loc[:,\n",
    "                          ['end_lat', 'end_lon', 'purpose_confirm']].dropna()\n",
    "c4 = shankari.loc[(shankari[f'{loc_type}_{alg}_clusters_150_m'] == 4\n",
    "                   )].loc[:,\n",
    "                          ['end_lat', 'end_lon', 'purpose_confirm']].dropna()\n",
    "\n",
    "clusters = [c1, c2, c3, c4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4[['purpose_confirm']].value_counts() / len(c4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kernel comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, I think it makes sense to use an RBF kernel since we expect our clusters to be shaped round-ish-ly. Let's do a quick comparison just to see the different results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decision functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(clusters)):\n",
    "    cluster = clusters[c]\n",
    "    # setup up model\n",
    "    X = cluster.loc[:, ['end_lon', 'end_lat']]\n",
    "    Y = cluster.loc[:, 'purpose_confirm'].to_list()\n",
    "    # fit() wants Y as an array, not a column vector\n",
    "\n",
    "    linear_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='linear', C=1, decision_function_shape='ovr'))\n",
    "    radial_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='rbf', gamma=0.1, C=1, decision_function_shape='ovr'))\n",
    "    polynom2_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='poly', degree=2, C=1, decision_function_shape='ovr'))\n",
    "    polynom4_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='poly', degree=4, C=1, decision_function_shape='ovr'))\n",
    "\n",
    "    models = [linear_model, radial_model, polynom2_model, polynom4_model]\n",
    "    model_names = [\n",
    "        'linear_kernel', 'radial_kernel', 'polynom2_kernel', 'polynom4_kernel'\n",
    "    ]\n",
    "\n",
    "    # vars for visualizing decision functions\n",
    "    min_lat = X[['end_lat']].min()\n",
    "    max_lat = X[['end_lat']].max()\n",
    "    min_lon = X[['end_lon']].min()\n",
    "    max_lon = X[['end_lon']].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(min_lon, max_lon, 500),\n",
    "                         np.linspace(min_lat, max_lat, 500))\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        model_name = model_names[i]\n",
    "        print(f'model {model_name} for cluster {c}')\n",
    "\n",
    "        # train models\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        num_classes = len(model.classes_)\n",
    "        decisions = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        decisions = decisions.reshape((xx.shape[0], xx.shape[1], num_classes))\n",
    "\n",
    "        fig, axs = plt.subplots(num_classes // 2 + num_classes % 2,\n",
    "                                2,\n",
    "                                sharex=True,\n",
    "                                sharey=True,\n",
    "                                figsize=(6, 6))\n",
    "\n",
    "        axs = axs.flatten()\n",
    "        for i in range(num_classes):\n",
    "            axs[i].set_title(model.classes_[i])\n",
    "            axs[i].imshow(\n",
    "                decisions[:, :, i],\n",
    "                interpolation=\"nearest\",\n",
    "                extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                aspect=\"auto\",\n",
    "                origin=\"lower\",\n",
    "            )\n",
    "\n",
    "            axs[i].scatter(\n",
    "                X['end_lon'],\n",
    "                X['end_lat'],\n",
    "                # c=Y['purpose_confirm'].map(colors).to_list(),\n",
    "                edgecolors=\"k\")\n",
    "        plt.axis('scaled')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot actual label predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(clusters)):\n",
    "    cluster = clusters[c]\n",
    "    # setup up model\n",
    "    X = cluster.loc[:, ['end_lon', 'end_lat']]\n",
    "    Y = cluster.loc[:, 'purpose_confirm'].to_list()\n",
    "    # fit() wants Y as an array, not a column vector\n",
    "\n",
    "    linear_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='linear', C=1, decision_function_shape='ovr'))\n",
    "    radial_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='rbf', gamma=0.1, C=1, decision_function_shape='ovr'))\n",
    "    polynom2_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='poly', degree=2, C=1, decision_function_shape='ovr'))\n",
    "    polynom4_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='poly', degree=4, C=1, decision_function_shape='ovr'))\n",
    "\n",
    "    models = [linear_model, radial_model, polynom2_model, polynom4_model]\n",
    "    model_names = [\n",
    "        'linear_kernel', 'radial_kernel', 'polynom2_kernel', 'polynom4_kernel'\n",
    "    ]\n",
    "\n",
    "    # vars for visualizing decision functions\n",
    "    min_lat = X[['end_lat']].min()\n",
    "    max_lat = X[['end_lat']].max()\n",
    "    min_lon = X[['end_lon']].min()\n",
    "    max_lon = X[['end_lon']].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(min_lon, max_lon, 500),\n",
    "                         np.linspace(min_lat, max_lat, 500))\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        model_name = model_names[i]\n",
    "        print(f'model {model_name} for cluster {c}')\n",
    "\n",
    "        # train models\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        num_classes = len(model.classes_)\n",
    "        label_map = {model.classes_[i]: i for i in range(num_classes)}\n",
    "\n",
    "        pred = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        pred = [label_map[p] for p in pred]\n",
    "        pred = np.array(pred).reshape((xx.shape[0], xx.shape[1]))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        im = ax.imshow(pred,\n",
    "                       interpolation=\"none\",\n",
    "                       extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                       aspect=\"auto\",\n",
    "                       origin=\"lower\",\n",
    "                       cmap=plt.cm.tab10,\n",
    "                       vmin=0,\n",
    "                       vmax=9)\n",
    "\n",
    "        ax.scatter(\n",
    "            X['end_lon'],\n",
    "            X['end_lat'],\n",
    "            # s=30,\n",
    "            # c=Y['purpose_confirm'].map(colors).to_list(),\n",
    "            edgecolors=\"k\")\n",
    "        plt.axis('scaled')\n",
    "        plt.tight_layout()\n",
    "        # plt.legend()\n",
    "        print(label_map)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, we definitely want to use a radial kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should do some hyperparameter tuning, but tbh these default values are doing pretty well so I'm going to push that off for later. \n",
    "\n",
    "A bigger concern: sometimes, the cluster for two labels naturally overlap - for example, home may overlap with roundtrips because the end destination is exactly the same. SVM is going to attempt to distinguish between these but epicly fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for a demo of the 'overlap problem'. In the map, notice how orange 'home' points totally overlap with green 'pick_drop_person' points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mapping.plot_clusters(\n",
    "    expanded_all_trip_df_map[0],\n",
    "    alg='DBSCAN',\n",
    "    loc_type='end',\n",
    "    # cluster_unlabeled=True,\n",
    "    # plot_unlabeled=True,\n",
    "    radii=[150])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at how SVM attempts to separate the sub-clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just copying the above code into a convenient function\n",
    "def vis_pred(cluster):\n",
    "    # setup up model\n",
    "    X = cluster.loc[:, ['end_lon', 'end_lat']]\n",
    "    Y = cluster.loc[:, 'purpose_confirm'].to_list()\n",
    "    # fit() wants Y as an array, not a column vector\n",
    "\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(kernel='rbf', gamma=0.05, C=1, decision_function_shape='ovr'))\n",
    "\n",
    "    # train models\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # vars for visualizing decision functions\n",
    "    min_lat = X[['end_lat']].min()\n",
    "    max_lat = X[['end_lat']].max()\n",
    "    min_lon = X[['end_lon']].min()\n",
    "    max_lon = X[['end_lon']].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(min_lon, max_lon, 500),\n",
    "                         np.linspace(min_lat, max_lat, 500))\n",
    "\n",
    "    num_classes = len(model.classes_)\n",
    "    label_map = {model.classes_[i]: i for i in range(num_classes)}\n",
    "\n",
    "    pred = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    print(np.unique(np.array(pred)))\n",
    "    pred = [label_map[p] for p in pred]\n",
    "    pred = np.array(pred).reshape((xx.shape[0], xx.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    im = ax.imshow(pred,\n",
    "                   interpolation=\"none\",\n",
    "                   extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                   aspect=\"auto\",\n",
    "                   origin=\"lower\",\n",
    "                   cmap=plt.cm.tab10,\n",
    "                   vmin=0,\n",
    "                   vmax=9)\n",
    "\n",
    "    ax.scatter(\n",
    "        X['end_lon'],\n",
    "        X['end_lat'],\n",
    "        # s=30,\n",
    "        # c=Y['purpose_confirm'].map(colors).to_list(),\n",
    "        edgecolors=\"k\")\n",
    "    plt.axis('scaled')\n",
    "    plt.tight_layout()\n",
    "    # plt.legend()\n",
    "    print(label_map)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pred(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple comments: \n",
    "- there are only 3 unique labels in the output prediction for this cluster (out of 7 input labels). This is nice, because we may have some rare labels that we mostly want to ignore for now, and we won't have to pre-filter them because SVM already takes care of it. \n",
    "- SVM is able to distinguish between shopping and home, which is good. However, it can't distinguish between home and pick_drop because those destinations are inherently the same place. I think this is ok because we can just handle that using other trip features later. The concern would only be if SVM attempted to actually separate home and pick_drop (which could potentially happen if we choose bad hyperparameters or our data is sparse/noisy.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, let's visualize the decision boundaries for some more clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pred(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pred(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pred(c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline! \n",
    "\n",
    "New pipeline:\n",
    "- make clusters via DBSCAN (containing both labeled and unlabeled trips)\n",
    "- - If we use SVM, we want to use DBSCAN because that creates better clusters that actually revolve around density cores. Oursim frequently has the issue of splitting up density cores because an outlier was added to the bin early on. (note to self, this is not a good explanation, clarify later)\n",
    "- if cluster has enough points and a low enough purity, then fit an SVM using labeled data\n",
    "- then, use SVM to make predictions on the unlabeled (using just their lat lon data)\n",
    "- (so calling it a second round of 'clustering' isn't quite the correct term, but oh well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test to make sure pipeline is working\n",
    "\n",
    "fig = mapping.plot_clusters(expanded_labeled_trip_df_map[0],\n",
    "                            alg='SVM',\n",
    "                            loc_type='end',\n",
    "                            cluster_unlabeled=False,\n",
    "                            plot_unlabeled=True,\n",
    "                            radii=[100, 150, 200, 250, 300, 500])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with original DBSCAN results\n",
    "\n",
    "fig = mapping.plot_clusters(expanded_labeled_trip_df_map[0],\n",
    "                            alg='DBSCAN',\n",
    "                            loc_type='end',\n",
    "                            cluster_unlabeled=False,\n",
    "                            plot_unlabeled=True,\n",
    "                            radii=[100, 150, 200, 250, 300, 500])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, its working! and it seems to be doing decently well, actually. For instance, at 150m, the SVM was able to split the home/shopping clusters and library/shopping clusters that DBSCAN had merged. At 200m, it was able to separate home, library and shopping. The shopping cluster *looks* pretty weird, because there are inherently two separate shopping clusters - but SVM gave them the same 'shopping' label and thus we drew an elongated convex hull. However, if we look above, back to where we plotted the prediction boundaries, we see two separate islands of shopping. So it's lacking a little bit in principle - but would probably perform fine in predicting purpose. It's possible, though, that the two clusters could have different modes since they're located in different places (not in this data, but possible in general).\n",
    "\n",
    "Also, there some overfitting with 1-2 trip library clusters at 250m (but this is gone at 300m, weird). We also have an issue at 400m+ that the big shopping cluster is not being split up, maybe because the impurity threshold is too low? \n",
    "\n",
    "ok now the real test: will the ***same*** alg+hyperparameters work for *my* data, which is on a college campus where all the buildings are much closer to each other? (This is the main issue with all the other clustering algorithms I've tried so far – I can hardcode good hyperparameters for an individual user/region, but they may not work for other regions – e.g. sprawling suburbs vs dense college campus or city downtown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mapping.plot_clusters(expanded_labeled_trip_df_map[2],\n",
    "                            alg='SVM',\n",
    "                            loc_type='end',\n",
    "                            cluster_unlabeled=False,\n",
    "                            plot_unlabeled=True,\n",
    "                            radii=[100, 150, 200, 250, 300, 500])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan for user2\n",
    "\n",
    "fig = mapping.plot_clusters(expanded_labeled_trip_df_map[2],\n",
    "                            alg='DBSCAN',\n",
    "                            loc_type='end',\n",
    "                            cluster_unlabeled=False,\n",
    "                            plot_unlabeled=True,\n",
    "                            radii=[100, 150, 200, 250, 300, 500])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... so it definitely did help, but still isn't the most optimal. ~~For 150m, it was able to split meal/school, which DBSCAN had merged. At 200m, it was able to split ac end from res end, which DBSCAN had merged; however, it didn't do any subsplitting of ac end. Maybe we want repeated iterations of SVM. This should be feasible, since we would use the same purity threshold.~~ Subsplitting is now implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omg, it worked. though maybe it overfit itself for 200m (like, why is there a cluster with a single lonely meal trip, sitting right next to a big meal cluster?). Also, it doesn't really fix the issue that some clusters have too-large diameters, but few enough points/high enough purity that SVM is not triggered (or maybe SVM is triggered, but isn't able to distinguish between them). I guess this is something we can try and address by tuning the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mapping.plot_clusters(expanded_labeled_trip_df_map[1],\n",
    "                            alg='SVM',\n",
    "                            loc_type='end',\n",
    "                            cluster_unlabeled=False,\n",
    "                            plot_unlabeled=True,\n",
    "                            radii=[100, 150, 200, 250, 300, 500])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters to tune:\n",
    "- cluster size threshold\n",
    "- purity threshold\n",
    "- gamma (size of RBF kernel)\n",
    "- C (boundary softness/regularization)\n",
    "\n",
    "The SVM is currently using the following parameters, which are yielding decent results but were chosen somewhat arbitrarily: \n",
    "- size threshold = 6\n",
    "- purity threshold = 0.7\n",
    "- gamma = 0.05\n",
    "- C = 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39792147defedce75a7e3a68ae8b893956023a509c7f6b059d8d59165c20ef2c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
