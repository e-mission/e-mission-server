{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finnish-despite",
   "metadata": {},
   "source": [
    "# Explore multiple datasets\n",
    "\n",
    "In this notebook, we are going to experiment with characterising the three datasets that we have in terms of data quality and demographic characteristics.\n",
    "\n",
    "This notebook is intended to be run on the exported, federated csv file. The file should be exported using `Federating and saving multiple datasets.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-seventh",
   "metadata": {},
   "source": [
    "### First, we read the data and extract the most common purpose labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geojson as gj\n",
    "import sklearn.cluster as sc\n",
    "import sklearn.metrics.pairwise as smp\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.element as bre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as pltc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from uuid import UUID\n",
    "\n",
    "import bson.json_util as bju\n",
    "import bson.objectid as boi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.decorations.trip_queries as esdtq\n",
    "import emission.analysis.modelling.tour_model.similarity as eamts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emission.core.wrapper.entry as ecwe\n",
    "import emission.core.wrapper.confirmedtrip as ecwct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-webcam",
   "metadata": {},
   "source": [
    "### Read data and setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_expanded_df = pd.read_json(open(\"/tmp/federated_trip_only_dataset.json\"), orient=\"records\", typ=\"frame\")\n",
    "for id_col in [\"_id\", \"raw_trip\", \"start_place\", \"end_place\", \"cleaned_trip\"]:\n",
    "    all_expanded_df[id_col] = all_expanded_df[id_col].apply(lambda i: boi.ObjectId(i[\"$oid\"]))\n",
    "    \n",
    "all_expanded_df[\"user_id\"] = all_expanded_df[\"user_id\"].apply(lambda u: UUID(u[\"$uuid\"]))\n",
    "all_expanded_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_expanded_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_program(user_id):\n",
    "    all_programs = all_expanded_df[all_expanded_df.user_id == user_id][\"program\"].unique()\n",
    "    assert len(all_programs) == 1, f\"all_programs = {all_programs}\"\n",
    "    return all_programs[0]\n",
    "\n",
    "participant_df = pd.DataFrame(all_expanded_df.user_id.unique(), columns=[\"user_id\"])\n",
    "participant_df = participant_df[participant_df.user_id != 0]\n",
    "participant_df.set_index(\"user_id\", inplace=True, drop=True)\n",
    "participant_df[\"program\"] = [get_unique_program(u) for u in participant_df.index]\n",
    "participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_support_objects = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RADIUS = 500\n",
    "FINAL_FILTER_OURSIM = False\n",
    "FINAL_CUTOFF_OURSIM = False\n",
    "FINAL_POINT_DBSCAN = sc.DBSCAN(FINAL_RADIUS, min_samples=2, metric=\"precomputed\")\n",
    "FINAL_TRIP_DBSCAN = sc.DBSCAN(FINAL_RADIUS * 2, min_samples=2, metric=\"precomputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-modern",
   "metadata": {},
   "source": [
    "### Standard functions (currently copied over from other notebooks; should be refactored into a python file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_df(loc_series):\n",
    "    loc_df = pd.DataFrame(loc_series.apply(lambda p: p[\"coordinates\"]).to_list(), columns=[\"longitude\", \"latitude\"])\n",
    "    # display.display(end_loc_df.head())\n",
    "    return loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(loc_df):\n",
    "    EARTH_RADIUS = 6371000\n",
    "    radians_lat_lon = np.radians(loc_df[[\"latitude\", \"longitude\"]])\n",
    "    dist_matrix_meters = pd.DataFrame(smp.haversine_distances(radians_lat_lon, radians_lat_lon) * 6371000)\n",
    "    return dist_matrix_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_loc_clusters(user_id, modeling_support_objects, trip_df):\n",
    "    user_trip_df = trip_df[trip_df.user_id == user_id]\n",
    "    start_distance_matrix = get_distance_matrix(get_loc_df(user_trip_df.start_loc))\n",
    "    end_distance_matrix = get_distance_matrix(get_loc_df(user_trip_df.end_loc))\n",
    "    start_loc_model = copy.copy(FINAL_POINT_DBSCAN).fit(start_distance_matrix)\n",
    "    end_loc_model = copy.copy(FINAL_POINT_DBSCAN).fit(end_distance_matrix)\n",
    "    trip_df.loc[user_trip_df.index, \"start_loc_cluster\"] = start_loc_model.labels_\n",
    "    trip_df.loc[user_trip_df.index, \"end_loc_cluster\"] = end_loc_model.labels_\n",
    "\n",
    "    curr_model_support = modeling_support_objects.get(user_id)\n",
    "    if curr_model_support is None:\n",
    "        modeling_support_objects[user_id] = {}\n",
    "        curr_model_support = modeling_support_objects[user_id]\n",
    "    curr_model_support[\"start_distance_matrix\"] = start_distance_matrix\n",
    "    curr_model_support[\"end_distance_matrix\"] = end_distance_matrix   \n",
    "    curr_model_support[\"start_loc_model\"] = start_loc_model\n",
    "    curr_model_support[\"end_loc_model\"] = end_loc_model\n",
    "\n",
    "    return trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trip_clusters_dbscan(user_id, trip_df):\n",
    "    user_trip_df = trip_df[trip_df.user_id == user_id]\n",
    "    all_combos = user_trip_df.groupby([\"start_loc_cluster\", \"end_loc_cluster\"])\n",
    "    valid_combos = [p for p in all_combos.groups if p[0] != -1 and p[1] != -1]\n",
    "    print(f\"After validating, all_combos {len(all_combos.groups)} -> {len(valid_combos)}\")\n",
    "    all_combos_dict = dict(all_combos.groups)\n",
    "    valid_combos_series = pd.Series(valid_combos)\n",
    "    for g, idxlist in all_combos_dict.items():\n",
    "        print(g, idxlist)\n",
    "        match = valid_combos_series[valid_combos_series == g]\n",
    "        if len(match) == 0:\n",
    "            print(f\"invalid combo {g} found for entries {idxlist}, trip is not in a cluster\")\n",
    "            trip_df.loc[idxlist, \"trip_cluster_dbscan\"] = -1\n",
    "        else:\n",
    "            print(f\"valid combo {g} found for entries {idxlist}, setting trip cluster to {match.index[0]}\")\n",
    "            trip_df.loc[idxlist, \"trip_cluster_dbscan\"] = int(match.index[0])\n",
    "    return trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269566f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ground_truth(trip_df, columns, gt_label):\n",
    "    unique_tuples = dict(trip_df.groupby(by=columns).groups)\n",
    "    for i, idxlist in enumerate(unique_tuples.values()):\n",
    "    # print(i, idxlist)\n",
    "        trip_df.loc[idxlist, gt_label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trip_clusters_oursim(user_id, modeling_support_objects, trip_df):\n",
    "    user_trip_df = trip_df[trip_df.user_id == user_id]\n",
    "    user_trip_list = [ecwe.Entry({\"data\": ecwct.Confirmedtrip(tr), \"_id\": tr[\"_id\"], \"metadata\": {\"key\": \"analysis/confirmed_trip\"}}) for tr in user_trip_df.to_dict(\"records\")]\n",
    "    curr_sim = eamts.similarity(user_trip_list, FINAL_RADIUS, shouldFilter=FINAL_FILTER_OURSIM, cutoff=FINAL_CUTOFF_OURSIM)\n",
    "    curr_sim.fit()\n",
    "    trip_df.loc[user_trip_df.index, \"trip_cluster_oursim\"] = curr_sim.labels_.to_list()\n",
    "    modeling_support_objects[user_id][\"similarity_model\"] = curr_sim\n",
    "    return trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70afa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_score_no_na(labels_true, labels_pred):\n",
    "    na_index = labels_true[pd.isna(labels_true)].index\n",
    "    # Before we set the index to nan; we don't want to have a side effect here!\n",
    "    new_labels_pred = labels_pred.copy()\n",
    "    new_labels_pred.loc[na_index] = np.nan\n",
    "    if (len(na_index) > 0):\n",
    "        print(f\"Dropping nan indices {na_index} before calculating score\")\n",
    "        # print(f\"{labels_true.dropna()}, {new_labels_pred.dropna()}\")\n",
    "    return sm.homogeneity_score(labels_true.dropna(), new_labels_pred.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8897a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_count(labels_pred):\n",
    "    # once per real cluster\n",
    "    # once per noisy point (since it is not in a cluster)\n",
    "    # once per filtered trip (not really necessary for our current regime, but good to be prepared)\n",
    "    return np.count_nonzero(labels_pred.unique() >= 0) \\\n",
    "                    + np.count_nonzero(labels_pred == -1) \\\n",
    "                    + np.count_nonzero(labels_pred == -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_basic_stats(user_id, participant_df, trip_df):\n",
    "    user_trip_df = trip_df[trip_df.user_id == user_id]\n",
    "    basic_stats = {}\n",
    "    basic_stats[\"n_labeled_trips\"] = len(user_trip_df)\n",
    "    basic_stats[\"unique_label_combos\"] = list(user_trip_df.groupby([\"mode_confirm\", \"purpose_confirm\", \"replaced_mode\"]).groups)\n",
    "    basic_stats[\"start_loc_in_cluster\"] = np.count_nonzero(user_trip_df.start_loc_cluster != -1)\n",
    "    basic_stats[\"end_loc_in_cluster\"] = np.count_nonzero(user_trip_df.end_loc_cluster != 1)\n",
    "    for algo in [\"dbscan\", \"oursim\"]:\n",
    "        print(f\"Generating basic stats for {user_id} {len(user_trip_df)}, algo {algo} \")\n",
    "        basic_stats[f\"n_trip_in_cluster_{algo}\"] = np.count_nonzero(user_trip_df[f\"trip_cluster_{algo}\"] != -1)\n",
    "        # need to add one to the max, since the cluster labels start at 0\n",
    "        basic_stats[f\"n_clusters_{algo}\"] = user_trip_df[f\"trip_cluster_{algo}\"].max() + 1\n",
    "        basic_stats[f\"cluster_trip_ratio_{algo}\"] = basic_stats[f\"n_clusters_{algo}\"] / basic_stats[f\"n_trip_in_cluster_{algo}\"]\n",
    "        basic_stats[f\"homogeneity_score_{algo}\"] = h_score_no_na(user_trip_df.ground_truth_by_tuple, user_trip_df[f\"trip_cluster_{algo}\"])\n",
    "        basic_stats[f\"request_count_{algo}\"] = request_count(user_trip_df[f\"trip_cluster_{algo}\"])\n",
    "        basic_stats[f\"request_pct_{algo}\"] = basic_stats[f\"request_count_{algo}\"] / basic_stats[\"n_labeled_trips\"]\n",
    "\n",
    "    # print(f\"Adding cols {basic_stats.keys()} with vals {basic_stats.values()}\")\n",
    "    participant_df.loc[user_id, basic_stats.keys()] = basic_stats.values()\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-vertex",
   "metadata": {},
   "source": [
    "Target exploratory analysis:\n",
    "\n",
    "- number of users\n",
    "- number of trips\n",
    "- labeled trip/user distribution\n",
    "- number of unique combinations of labels\n",
    "- distribution of unique combination of labels (overall)\n",
    "- distribution of unique combination of labels (per-user)\n",
    "- number of trips whose end point is in a cluster\n",
    "- number of trips whose start point is in a cluster\n",
    "- number of trips where trip is in a cluster\n",
    "- number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in participant_df.index:\n",
    "    all_expanded_df = add_trip_clusters_dbscan(u, add_loc_clusters(u, modeling_support_objects,all_expanded_df))\n",
    "    add_trip_clusters_oursim(u, modeling_support_objects, all_expanded_df)\n",
    "    add_ground_truth(all_expanded_df, [\"mode_confirm\", \"purpose_confirm\", \"replaced_mode\"], \"ground_truth_by_tuple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c648dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in participant_df.index:\n",
    "    update_basic_stats(u, participant_df, all_expanded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-salmon",
   "metadata": {},
   "source": [
    "### Again, let's focus on one dataset before generalizing to other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "minipilot_df = participant_df[participant_df.program == \"minipilot\"]\n",
    "minipilot_df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "minipilot_df[[\"n_labeled_trips\", \"start_loc_in_cluster\", \"end_loc_in_cluster\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\"]].plot(kind=\"bar\", figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-muscle",
   "metadata": {},
   "source": [
    "# Final results, generalized to the entire dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-browse",
   "metadata": {},
   "source": [
    "### First, let's just display everything, without grouping by program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-responsibility",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = participant_df[[\"n_labeled_trips\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\", \"request_count_dbscan\", \"request_count_oursim\"]].plot(kind=\"bar\", use_index=False, figsize=(30,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-petite",
   "metadata": {},
   "source": [
    "### Next, let's group by dataframe to see if there are consistent program level differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_df[participant_df.program == \"minipilot\"][[\"n_labeled_trips\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\", \"request_count_dbscan\", \"request_count_oursim\"]].plot(kind=\"bar\", figsize=(20,5), use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_df[participant_df.program == \"nrel_lh\"][[\"n_labeled_trips\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\", \"request_count_dbscan\", \"request_count_oursim\"]].plot(kind=\"bar\", figsize=(20,5), use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_df[participant_df.program == \"stage\"][[\"n_labeled_trips\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\", \"request_count_dbscan\", \"request_count_oursim\"]].plot(kind=\"bar\", figsize=(20,5), use_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-rouge",
   "metadata": {},
   "source": [
    "### Assessing clustering effectiveness\n",
    "\n",
    "We use our standard metrics to assess the tradeoff between request pct and homogeneity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,5))\n",
    "axarr = fig.subplots(1,2)\n",
    "colors = plt.get_cmap(\"Accent\", 4).colors\n",
    "\n",
    "for i, algo in enumerate([\"dbscan\", \"oursim\"]):\n",
    "    ax = axarr[i]\n",
    "    # ax = result_df.plot.scatter(x=f\"no_filter_no_cutoff_{r}_homogeneity_score_tuple\", y=f\"no_filter_no_cutoff_{r}_request_pct\", label=f\"no_filter_no_cutoff_{r}\")\n",
    "    for j, p in enumerate(participant_df.program.unique()):\n",
    "        curr_p_df = participant_df[participant_df.program==p]\n",
    "        curr_p_df.plot.scatter(x=f\"homogeneity_score_{algo}\", y=f\"request_pct_{algo}\", color=colors[j], label=f\"{p}\", ax=ax)\n",
    "    ax.set_xlabel(\"homogeneity score\")\n",
    "    ax.set_ylabel(\"request pct\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d10ad",
   "metadata": {},
   "source": [
    "The request percents on for both the DBSCAN (left) and oursim (right) are similar, but the homogeneity for oursim is clearly better. In fact, we can see a clear linear trend in which a higher request pct leads to better scores. Again, this is an indication that single trip clusters are dominating our results. This is also likely to involve some outliers in which we have very little data to work with.\n",
    "\n",
    "Let's re-plot with the number of trips to get a sense of that correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,5))\n",
    "axarr = fig.subplots(1,2)\n",
    "colors = plt.get_cmap(\"Accent\", 4).colors\n",
    "\n",
    "for i, algo in enumerate([\"dbscan\", \"oursim\"]):\n",
    "    ax = axarr[i]\n",
    "    participant_df.plot.scatter(x=f\"homogeneity_score_{algo}\", y=f\"request_pct_{algo}\", c=\"n_labeled_trips\", cmap=\"viridis\", ax=ax)\n",
    "    ax.set_xlabel(\"homogeneity score\")\n",
    "    ax.set_ylabel(\"request pct\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f00d4",
   "metadata": {},
   "source": [
    "Since we appear to have a linear relationship between the h-score and the request pct, let's use one of the axes for the number of trips, to see if the visualization becomes more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,5))\n",
    "axarr = fig.subplots(1,2)\n",
    "colors = plt.get_cmap(\"Accent\", 4).colors\n",
    "\n",
    "for i, algo in enumerate([\"dbscan\", \"oursim\"]):\n",
    "    ax = axarr[i]\n",
    "    for j, p in enumerate(participant_df.program.unique()):\n",
    "        curr_p_df = participant_df[participant_df.program==p]\n",
    "        curr_p_df.plot.scatter(x=f\"n_labeled_trips\", y=f\"request_pct_{algo}\", ax=ax, color=colors[j], label=p)\n",
    "    ax.set_xlabel(\"number of labeled trips\")\n",
    "    ax.set_ylabel(\"request pct\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6f52e",
   "metadata": {},
   "source": [
    "The linear scale is resulting in a bunch of points clustered around the edge of the X-axis, with little differentiation between them. Let's try a log scale instead to see if it makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,5))\n",
    "axarr = fig.subplots(1,2)\n",
    "colors = plt.get_cmap(\"Accent\", 4).colors\n",
    "\n",
    "for i, algo in enumerate([\"dbscan\", \"oursim\"]):\n",
    "    ax = axarr[i]\n",
    "    for j, p in enumerate(participant_df.program.unique()):\n",
    "        curr_p_df = participant_df[participant_df.program==p]\n",
    "        curr_p_df.plot.scatter(x=f\"n_labeled_trips\", y=f\"request_pct_{algo}\", ax=ax, color=colors[j], label=p, logx=True)\n",
    "    ax.set_xlabel(\"number of trips (log scale)\")\n",
    "    ax.set_ylabel(\"request pct\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4fcaa",
   "metadata": {},
   "source": [
    "This does appear to show a reasonably clear trend. If we have less than 10 trips, we cannot see any patterns; request_pct = 1. Between 10 and 100 trips, there is a weak trend towards more data providing better results, but it appears to be linear on the log scale, so there is likely some diminishing returns.\n",
    "\n",
    "Speaking of trends, let us look at more statistical visualizations of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(10,10))\n",
    "axarr = fig.subplots(2,2,sharex=True, sharey=True)\n",
    "participant_df.boxplot(\"request_pct_dbscan\", by=\"program\", ax=axarr[0][0])\n",
    "participant_df.boxplot(\"request_pct_oursim\", by=\"program\", ax=axarr[0][1])\n",
    "participant_df.boxplot(\"homogeneity_score_dbscan\", by=\"program\", ax=axarr[1][0])\n",
    "participant_df.boxplot(\"homogeneity_score_oursim\", by=\"program\", ax=axarr[1][1])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa514b2",
   "metadata": {},
   "source": [
    "Again, the request percentages for the programs are similar, but the homogeneity score is significantly better for oursim. Based on the results above, let's plot separately for nTrips > 10 and nTrips > 100, for oursim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(10,10))\n",
    "axarr = fig.subplots(2,2,sharex=True, sharey=True)\n",
    "participant_df[participant_df.n_labeled_trips > 10].boxplot(\"request_pct_oursim\", by=\"program\", ax=axarr[0][0])\n",
    "axarr[0][0].set_title(\"num labeled trip > 10\")\n",
    "participant_df[participant_df.n_labeled_trips > 100].boxplot(\"request_pct_oursim\", by=\"program\", ax=axarr[0][1])\n",
    "axarr[0][1].set_title(\"num labeled trip > 100\")\n",
    "participant_df[participant_df.n_labeled_trips > 10].boxplot(\"homogeneity_score_oursim\", by=\"program\", ax=axarr[1][0])\n",
    "axarr[1][0].set_title(\"num labeled trip > 10\")\n",
    "participant_df[participant_df.n_labeled_trips > 100].boxplot(\"homogeneity_score_oursim\", by=\"program\", ax=axarr[1][1])\n",
    "axarr[1][1].set_title(\"num labeled trip > 100\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb02e9e",
   "metadata": {},
   "source": [
    "As we can see, that tightens up the results considerably. There are now no outliers, and the IQR is also smaller. It looks like we end up with a mean request % between 0.3 and 0.5 and a mean h-score between 0.5 and 0.7, which seems pretty decent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9b892",
   "metadata": {},
   "source": [
    "### Old estimates using cluster count\n",
    "\n",
    "Finally, I want to spend a little bit of time focusing on the difference between the request % and the cluster trip ratio to fully understand what is going on. \n",
    "\n",
    "I originally came up with the cluster_trip ratio as an evaluation of the \"tightness\" of the cluster. Trips with a DBSCAN clustering are basically either in a cluster or noise. I found the ratio of the number of clusters to the number of trips in a cluster. If this was low, we would expect few requests, since most of the trips would be squished into a few clusters.\n",
    "\n",
    "However, the cluster trip ratio boxplot for DBSCAN looks a lot better than the request %. I originally thought that this may be because there might be a lot of noisy trips; note that the cluster trip ratio does not include trips that are *not* in clusters, while the request percentage does.\n",
    "\n",
    "However, if there are a lot of trips that are not in clusters, then the number of trips *in* the cluster should go down, so the denominator of the cluster_trip_ratio goes down, which increases the ratio. So why isn't that happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ec3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# participant_df[\"cluster_trip_ratio\"] = participant_df[\"n_clusters_dbscan\"] / participant_df[\"trip_in_cluster_dbscan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(10,3))\n",
    "axarr = fig.subplots(1,2,sharey=True)\n",
    "participant_df.boxplot(\"cluster_trip_ratio_dbscan\", by=\"program\", ax=axarr[0])\n",
    "participant_df.boxplot(\"cluster_trip_ratio_oursim\", by=\"program\", ax=axarr[1])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-invitation",
   "metadata": {},
   "source": [
    "The NREL LH program does in fact have a better cluster ratio overall than the other two programs. But even in the other two programs, most of the ratios are pretty low. Still, we can't help everybody, and there are going to be a small number of people who are going to have to label more than half their trips. Still, it is gratifying to see that the max overall is just a bit higher than 0.7.\n",
    "\n",
    "The same data with a slightly different visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using plt.scatter here instead of pandas.plot since it is non-trivial to use the index as the x axis\n",
    "# https://stackoverflow.com/questions/49834883/scatter-plot-form-dataframe-with-index-on-x-axis\n",
    "# x=df.index does not work for me, may be due to an older version of pandas\n",
    "color_list = plt.get_cmap(\"Accent\", 3).colors\n",
    "fig = plt.Figure(figsize=(10,5))\n",
    "ax = fig.subplots(1,1)\n",
    "for i, p in enumerate(participant_df.program.unique()):\n",
    "    curr_p_df = participant_df[participant_df.program==p]\n",
    "    ax.scatter([str(u) for u in curr_p_df.index], curr_p_df[\"cluster_trip_ratio_dbscan\"], color=color_list[i], label=p)\n",
    "ax.set_xticklabels(range(0,len(participant_df)))\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48cede4",
   "metadata": {},
   "source": [
    "### Entries with low cluster trip ratio and high request pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(20,5))\n",
    "axarr = fig.subplots(1,2)\n",
    "colors = plt.get_cmap(\"Accent\", 4).colors\n",
    "\n",
    "for i, algo in enumerate([\"dbscan\", \"oursim\"]):\n",
    "    ax = axarr[i]\n",
    "    for j, p in enumerate(participant_df.program.unique()):\n",
    "        curr_p_df = participant_df[participant_df.program==p]\n",
    "        # Focus on trips with > 10 entries for meaningful results\n",
    "        curr_p_df[curr_p_df.n_labeled_trips > 10].plot.scatter(x=f\"request_pct_{algo}\", y=f\"cluster_trip_ratio_{algo}\", ax=ax, color=colors[j], label=p)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352a785",
   "metadata": {},
   "source": [
    "While the oursim is essentially a straight line, there are more outliers for DBSCAN.\n",
    "let's look at some of the outliers for DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43adb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_df.query(\"request_pct_dbscan > 0.5 and cluster_trip_ratio_dbscan < 0.3 and n_labeled_trips > 10\")[[\"program\", \"n_labeled_trips\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"request_pct_dbscan\", \"cluster_trip_ratio_dbscan\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc1c5b",
   "metadata": {},
   "source": [
    "Looking at the entries with the biggest differences:\n",
    "- **71f21d53**:\n",
    "  - clusters (2) + trips outside cluster (34) = 36. So request pct is 36/48 = 0.75\n",
    "  - clusters (2) / trips in cluster (14) = 0.14\n",
    "  \n",
    "So basically, the original assumption was correct; it is the noisy points that make a difference. Basically, this happens in the case where there are many noisy clusters, but the points that are within the clusters, compress down well. In that case, the noisy trips carry a lot more weight in the request pct, and excluding them has a much higher impact.\n",
    "\n",
    "- **a3587c47**:\n",
    "  - clusters (2) + trips outside cluster (5) = 7. So request pct should be 7/12 = 0.58. **Why is this 0.66 instead?!**\n",
    "  - clusters (2) / trips in cluster (7) = 0.28\n",
    "  \n",
    "There was a mistake in the n_clusters calculation. I was using the max of the column, but the labels start from zero, so the number of clusters is actually the max + 1.\n",
    "\n",
    "Fixed in notebook. New calculation is:\n",
    "- **a3587c47**:\n",
    "  - clusters (3) + trips outside cluster (5) = 8. So request pct should be 8/12 = 0.66.\n",
    "  - clusters (3) / trips in cluster (7) = 0.42\n",
    "  \n",
    "- **71f21d53**:\n",
    "  - clusters (3) + trips outside cluster (34) = 37. So request pct is 37/48 = 0.77\n",
    "  - clusters (3) / trips in cluster (14) = 0.21\n",
    "  \n",
    "Let's verify this by plotting with the difference in trips as the hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de741cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(5,5))\n",
    "ax = fig.subplots(1,1)\n",
    "colors = plt.get_cmap(\"Accent\", 4).colors\n",
    "\n",
    "temp_p_df = participant_df.copy()\n",
    "temp_p_df[\"noisy_trip_pct\"] = temp_p_df.apply(lambda r: r[\"n_labeled_trips\"] - r[\"n_trip_in_cluster_dbscan\"], axis=1) / temp_p_df[\"n_labeled_trips\"]\n",
    "temp_p_df.plot.scatter(x=f\"request_pct_dbscan\", y=f\"cluster_trip_ratio_dbscan\", c=\"noisy_trip_pct\", cmap=\"viridis\", ax=ax)\n",
    "ax.set_xlabel(\"homogeneity score\")\n",
    "ax.set_ylabel(\"request pct\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b546d",
   "metadata": {},
   "source": [
    "Alas, not sure that visualization is very useful :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f90efa",
   "metadata": {},
   "source": [
    "## DBSCAN-only plots, do not use\n",
    "\n",
    "These are plots that were primarily created when we were looking at the DBSCAN clustering; they can still be used, but  some of the columns may not make as much sense any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_df[participant_df.program == \"minipilot\"][[\"n_labeled_trips\", \"start_loc_in_cluster\", \"end_loc_in_cluster\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\"]].plot(kind=\"bar\", figsize=(20,5), use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-bride",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "participant_df[participant_df.program == \"nrel_lh\"][[\"n_labeled_trips\", \"start_loc_in_cluster\", \"end_loc_in_cluster\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\"]].plot(kind=\"bar\", figsize=(20,5), use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-portugal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "participant_df[participant_df.program == \"stage\"][[\"n_labeled_trips\", \"start_loc_in_cluster\", \"end_loc_in_cluster\", \"n_trip_in_cluster_dbscan\", \"n_clusters_dbscan\", \"n_clusters_oursim\"]].plot(kind=\"bar\", figsize=(20,5), use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-passion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
