{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finnish-despite",
   "metadata": {},
   "source": [
    "# Evaluate the use of the similarity code for trip clustering\n",
    "\n",
    "In this notebook, we are going to dig deeper into the use of the similarity code for trip clustering. The original goal of the similarity code was to find and display common trips to users in a user interface. We plan to use it to automatically label common trips.\n",
    "\n",
    "The goals seem very similar, but are they? Or should our usage patterns for this new use case be subtly different?\n",
    "\n",
    "This notebook is intended to be run against the participant-only version of the CanBikeCO Jan 31 minipilot dataset.\n",
    "If you have the older version that includes data from non-participants as well, please replace\n",
    "\n",
    "```\n",
    "all_users = esta.TimeSeries.get_uuid_list()\n",
    "```\n",
    "\n",
    "with \n",
    "\n",
    "```\n",
    "participant_uuid_obj = list(edb.get_profile_db().find({\"install_group\": \"participant\"}, {\"user_id\": 1, \"_id\": 0}))\n",
    "all_users = [u[\"user_id\"] for u in participant_uuid_obj]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-seventh",
   "metadata": {},
   "source": [
    "### First, we read the data and extract the most common purpose labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geojson as gj\n",
    "import sklearn.cluster as sc\n",
    "import sklearn.metrics.pairwise as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.element as bre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as pltc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from uuid import UUID\n",
    "\n",
    "import bson.json_util as bju\n",
    "import bson.objectid as boi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.decorations.trip_queries as esdtq\n",
    "import emission.analysis.modelling.tour_model.similarity as eamts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emission.core.wrapper.entry as ecwe\n",
    "import emission.core.wrapper.confirmedtrip as ecwct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-webcam",
   "metadata": {},
   "source": [
    "### Read data and setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = esta.TimeSeries.get_uuid_list()\n",
    "confirmed_trip_df_map = {}\n",
    "labeled_trip_df_map = {}\n",
    "expanded_trip_df_map = {}\n",
    "for u in all_users:\n",
    "    ts = esta.TimeSeries.get_time_series(u)\n",
    "    ct_df = ts.get_data_df(\"analysis/confirmed_trip\")\n",
    "    confirmed_trip_df_map[u] = ct_df\n",
    "    labeled_trip_df_map[u] = esdtq.filter_labeled_trips(ct_df)\n",
    "    expanded_trip_df_map[u] = esdtq.expand_userinputs(labeled_trip_df_map[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RADIUS = 500\n",
    "FINAL_POINT_DBSCAN = sc.DBSCAN(FINAL_RADIUS, min_samples=2, metric=\"precomputed\")\n",
    "FINAL_TRIP_DBSCAN = sc.DBSCAN(FINAL_RADIUS * 2, min_samples=2, metric=\"precomputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-modern",
   "metadata": {},
   "source": [
    "### Standard functions (currently copied over from other notebooks; should be refactored into a python file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_df(loc_series):\n",
    "    loc_df = pd.DataFrame(loc_series.apply(lambda p: p[\"coordinates\"]).to_list(), columns=[\"longitude\", \"latitude\"])\n",
    "    # display.display(end_loc_df.head())\n",
    "    return loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(loc_df):\n",
    "    EARTH_RADIUS = 6371000\n",
    "    radians_lat_lon = np.radians(loc_df[[\"latitude\", \"longitude\"]])\n",
    "    dist_matrix_meters = pd.DataFrame(smp.haversine_distances(radians_lat_lon, radians_lat_lon) * 6371000)\n",
    "    return dist_matrix_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_loc_clusters(user_id, modeling_support_objects, trip_df):\n",
    "    user_trip_df = trip_df[trip_df.user_id == user_id]\n",
    "    start_distance_matrix = get_distance_matrix(get_loc_df(user_trip_df.start_loc))\n",
    "    end_distance_matrix = get_distance_matrix(get_loc_df(user_trip_df.end_loc))\n",
    "    start_loc_model = copy.copy(FINAL_POINT_DBSCAN).fit(start_distance_matrix)\n",
    "    end_loc_model = copy.copy(FINAL_POINT_DBSCAN).fit(end_distance_matrix)\n",
    "    trip_df.loc[user_trip_df.index, \"start_loc_cluster\"] = start_loc_model.labels_\n",
    "    trip_df.loc[user_trip_df.index, \"end_loc_cluster\"] = end_loc_model.labels_\n",
    "\n",
    "    curr_model_support = modeling_support_objects.get(user_id)\n",
    "    if curr_model_support is None:\n",
    "        modeling_support_objects[user_id] = {}\n",
    "        curr_model_support = modeling_support_objects[user_id]\n",
    "    curr_model_support[\"start_distance_matrix\"] = start_distance_matrix\n",
    "    curr_model_support[\"end_distance_matrix\"] = end_distance_matrix   \n",
    "    curr_model_support[\"start_loc_model\"] = start_loc_model\n",
    "    curr_model_support[\"end_loc_model\"] = end_loc_model\n",
    "\n",
    "    return trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trip_clusters_dbscan(user_id, trip_df):\n",
    "    user_trip_df = trip_df[trip_df.user_id == user_id]\n",
    "    all_combos = user_trip_df.groupby([\"start_loc_cluster\", \"end_loc_cluster\"])\n",
    "    valid_combos = [p for p in all_combos.groups if p[0] != -1 and p[1] != -1]\n",
    "    print(f\"After validating, all_combos {len(all_combos.groups)} -> {len(valid_combos)}\")\n",
    "    all_combos_dict = dict(all_combos.groups)\n",
    "    valid_combos_series = pd.Series(valid_combos)\n",
    "    for g, idxlist in all_combos_dict.items():\n",
    "        print(g, idxlist)\n",
    "        match = valid_combos_series[valid_combos_series == g]\n",
    "        if len(match) == 0:\n",
    "            print(f\"invalid combo {g} found for entries {idxlist}, trip is not in a cluster\")\n",
    "            trip_df.loc[idxlist, \"trip_cluster_dbscan\"] = -1\n",
    "        else:\n",
    "            print(f\"valid combo {g} found for entries {idxlist}, setting trip cluster to {match.index[0]}\")\n",
    "            trip_df.loc[idxlist, \"trip_cluster_dbscan\"] = int(match.index[0])\n",
    "    return trip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-standing",
   "metadata": {},
   "source": [
    "### First, we pick a participant to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trips_df = pd.DataFrame([[u, len(confirmed_trip_df_map[u]), len(labeled_trip_df_map[u])] for u in all_users], columns=[\"user_id\", \"all_trips\", \"labeled_trips\"]); n_trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user = n_trips_df[n_trips_df.labeled_trips == n_trips_df.labeled_trips.median()].user_id.iloc[0]; median_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_user_df = expanded_trip_df_map[median_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_trip_clusters_oursim(participant_df.index[0], all_expanded_df)\n",
    "user_id = median_user\n",
    "user_trip_df = median_user_df\n",
    "user_trip_list = [ecwe.Entry({\"data\": ecwct.Confirmedtrip(tr), \"_id\": tr[\"_id\"], \"metadata\": {\"key\": \"analysis/confirmed_trip\"}}) for tr in user_trip_df.to_dict(\"records\")]\n",
    "curr_sim = eamts.similarity(user_trip_list, FINAL_RADIUS)\n",
    "curr_sim.filter_trips()\n",
    "curr_sim.bin_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-absolute",
   "metadata": {},
   "source": [
    "### Step 1: filtering too short trips\n",
    "\n",
    "This is the first step of the binning, where we have not yet determined the cutoff and deleted the bins below it. However, we *have* filtered out trips that are too short. Let's examine if this filtering makes sense to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trip_df = pd.DataFrame([e[\"data\"] for e in curr_sim.data])\n",
    "removed_trips = user_trip_df[~user_trip_df._id.isin(filtered_trip_df._id)]\n",
    "removed_trips.distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-acquisition",
   "metadata": {},
   "source": [
    "Let's now characterise this in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_trips.distance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_trips.distance[removed_trips.distance > FINAL_RADIUS]; len(removed_trips.distance[removed_trips.distance > FINAL_RADIUS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_trips.boxplot(\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-thomson",
   "metadata": {},
   "source": [
    "### Filtering does not make sense\n",
    "\n",
    "It turns out that this filtering does not make sense.\n",
    "\n",
    "1. Even if the start and end are close to each other, the actual trip may not be very short because it may be a round trip.\n",
    "2. For example, around half of the trips that are filtered actually have a distance > our filter.\n",
    "3. The others may be \"not a trip\" aka fake trips. But those are still valuable! If the user has been labeling short trips to and from a particular location as \"Not a trip\", then automatically labeling those as \"Not a trip\" will still save the user a ton of time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-coordination",
   "metadata": {},
   "source": [
    "### Step 2: binning\n",
    "\n",
    "We now bin the trips to effectively create \"clusters\". Let us compare these clusters against the DBSCAN clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DBSCAN clusters\n",
    "modeling_support_objects = {}\n",
    "add_trip_clusters_dbscan(user_id, add_loc_clusters(user_id, modeling_support_objects, user_trip_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create oursim bin\n",
    "# curr_sim.bin_data()\n",
    "all_bins = curr_sim.bins\n",
    "for b in all_bins:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(curr_sim.data), len(curr_sim.filtered_data), len(curr_sim.all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-checkout",
   "metadata": {},
   "source": [
    "Let's try to convert the bins into labels to be consistent with the sklearn clustering algorithms\n",
    "**TODO: This should really go into the sim implementation**\n",
    "again, to be consistent with the sklearn configuration, we use a min_sample of 2\n",
    "so trips that are in a bin of length 1 are noise. the others are in a labeled cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_trips.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_trips), len(filtered_trip_df), len(user_trip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_all_bins = list(itertools.chain(*all_bins)); flat_all_bins[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_all_bins), max(flat_all_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by assuming that everything is noise, and set the ones that are not\n",
    "# This is a bit tricky wrt indexing, since the indices of the trips in the bin are after filtering,\n",
    "# so don't match up 1:1 with the indices in the trip dataframe\n",
    "# since we create a new dataframe for the filtered trips, they should match up with the filtered dataframe\n",
    "# but the index of the filtered dataframe is a new RangeIndex, so it doesn't work for indexing into the result series\n",
    "# so we need to follow a two-step process as below\n",
    "def get_result_labels(user_trip_df, filtered_trip_df):\n",
    "    \"\"\"\n",
    "    user_trip_df: all trips that we are determining similarity for\n",
    "    filtered_trip_df: trips filtered out by the \"too_short\" metric\n",
    "    \"\"\"\n",
    "    result_labels = pd.Series([-1] * len(user_trip_df))\n",
    "    for i, curr_bin in enumerate(all_bins):\n",
    "        if len(curr_bin) > 1:\n",
    "            # get the trip ids of matching filtered trips for the current bin\n",
    "            matching_filtered_trip_ids = filtered_trip_df.loc[curr_bin]._id\n",
    "            # then, match by tripid to find the corresponding entries in the all_trips dataframe\n",
    "            matching_all_trip_ids = user_trip_df[user_trip_df._id.isin(matching_filtered_trip_ids)].index\n",
    "            result_labels.loc[matching_all_trip_ids] = i\n",
    "            \n",
    "    removed_trips = user_trip_df[~user_trip_df._id.isin(filtered_trip_df._id)]\n",
    "    # For now, we also mark the \"too short\" labels with -2 to help with our understanding\n",
    "    result_labels.loc[removed_trips.index] = -2\n",
    "    return result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_trip_df[user_trip_df._id.isin(filtered_trip_df.loc[[15, 38, 45, 70, 98, 133]]._id)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels = get_result_labels(user_trip_df, filtered_trip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inline unit test\n",
    "all_bins[3], result_labels.loc[all_bins[3]], result_labels[result_labels == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(result_labels == -1), np.count_nonzero(result_labels == -2), np.count_nonzero(result_labels > 0), len(result_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_trip_df[\"trip_cluster_oursim\"] = get_result_labels(user_trip_df, filtered_trip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moment of truth: which has more noise? The custom binning algo, which is not surprising since it is fairly naive\n",
    "np.count_nonzero(user_trip_df.trip_cluster_dbscan == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-australia",
   "metadata": {},
   "source": [
    "### Our similarity code marks more trips as noisy\n",
    "\n",
    "This is not super surprising, since our implementation is a fairly naive one written by an undergrad over the summer, while DBSCAN is a (hopefully) more sophisticated and better performing algorithm from the literature.\n",
    "\n",
    "At this point, between the too short filtering (unnecessary and somewhat erroneous), which removed 49 trips, and the noisier binning (67 v/s 31), we have removed 116 trips out of our original 208 (around 55%). In contrast, the DBSCAN implementation has removed only 31 / 208 = 14%. This is a huge difference in terms of ongoing modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-trunk",
   "metadata": {},
   "source": [
    "### Step 3: Determining the cutoff\n",
    "\n",
    "Since we have already marked all bins of length 1 as noise, I don't anticipate this make very much of a difference. But let's finish it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_sim.delete_bins()\n",
    "above_cutoff_bins = curr_sim.bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "above_cutoff_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_all_bins = list(itertools.chain(*all_bins)); flat_all_bins[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(curr_sim.data), len(curr_sim.newdata), len(flat_all_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like the trips were not re-indexed here, so let's just go ahead with generating labels in the same way \n",
    "user_trip_df[\"trip_cluster_oursim_above_cutoff\"] = get_result_labels(user_trip_df, filtered_trip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As expected, for this user, there is no difference\n",
    "np.count_nonzero(user_trip_df.trip_cluster_dbscan == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-banner",
   "metadata": {},
   "source": [
    "### Step 4: Spot checking the noise\n",
    "\n",
    "Let's pick a trip that is noise in oursim, find its cluster in DBSCAN and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geojson_for_trip_cluster(cluster_trips):\n",
    "    print(len(cluster_trips))\n",
    "    # [[[X1, Y1], [X1, Y1]],\n",
    "    # [[X1, Y1], [X1, Y1]]]\n",
    "    clistarray = cluster_trips[[\"start_loc\", \"end_loc\"]].apply(\n",
    "                    lambda se: [p[\"coordinates\"] for p in se]).to_numpy().tolist()\n",
    "    print([len(clist) for clist in clistarray])\n",
    "    linestrings = [gj.LineString(coordinates=clist) for clist in clistarray]\n",
    "    purpose_locs = gj.FeatureCollection(cluster_trips.start_loc.to_list() +\n",
    "                                        cluster_trips.end_loc.to_list() +\n",
    "                                        linestrings)\n",
    "    return folium.features.GeoJson(purpose_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_noise_in_oursim = user_trip_df.query(\"(trip_cluster_oursim == -1) and (trip_cluster_dbscan != -1)\")\n",
    "extra_clusters = extra_noise_in_oursim.trip_cluster_dbscan.unique(); extra_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_trip_df[user_trip_df.trip_cluster_dbscan == extra_clusters[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = user_trip_df[user_trip_df.trip_cluster_dbscan == extra_clusters[-3]]; x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-copying",
   "metadata": {},
   "source": [
    "### Most of the \"non-noisy\" trips in the DBSCAN method have only one trip!!\n",
    "\n",
    "Since we cluster the start and locations separately (with min_samples = 2) but then just find unique combinations with them without requiring min_samples = 2 for the pair (the trip), we can end up with one trip and have it be non-noisy. This still seems principled - if both the start and the end are known places, even if there is only one trip between them, it is unlikely to represent an overlap with a different trip.\n",
    "\n",
    "But this does mean that our original criterion for what is noisy is too strict.\n",
    "On the other hand, step 3 would have filtered all those out as noisy anyway.\n",
    "\n",
    "So I don't think it changes the results for this user, but we should change our implementation to check for other users. Since this is unrolled, let's preserve the original result and copy-paste the new implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run all the similarity code again\n",
    "curr_sim = eamts.similarity(user_trip_list, FINAL_RADIUS)\n",
    "curr_sim.filter_trips()\n",
    "curr_sim.bin_data()\n",
    "filtered_trip_df = pd.DataFrame([e[\"data\"] for e in curr_sim.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_sim.data[0][\"_id\"], curr_sim.data[0][\"data\"][\"_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is an unrolled notebook, we redefine the function instead of modifying the previous implementation\n",
    "def get_result_labels_new(user_trip_df, filtered_trip_df):\n",
    "    \"\"\"\n",
    "    user_trip_df: all trips that we are determining similarity for\n",
    "    filtered_trip_df: trips retained after the \"too_short\" metric.\n",
    "    We need this because the model is a list of bins, and stores trip\n",
    "    indices based these filtered trips. But we want to set the results into the full user_trip_df.\n",
    "    \"\"\"\n",
    "    result_labels = pd.Series([-1] * len(user_trip_df))\n",
    "    for i, curr_bin in enumerate(curr_sim.bins):\n",
    "        # get the trip ids of matching filtered trips for the current bin\n",
    "        matching_filtered_trip_ids = filtered_trip_df.loc[curr_bin]._id\n",
    "        # then, match by tripid to find the corresponding entries in the all_trips dataframe\n",
    "        matching_all_trip_ids = user_trip_df[user_trip_df._id.isin(matching_filtered_trip_ids)].index\n",
    "        result_labels.loc[matching_all_trip_ids] = i\n",
    "            \n",
    "    removed_trips = user_trip_df[~user_trip_df._id.isin(filtered_trip_df._id)]\n",
    "    # For now, we also mark the \"too short\" labels with -2 to help with our understanding\n",
    "    result_labels.loc[removed_trips.index] = -2\n",
    "    return result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_trip_df[\"trip_cluster_oursim_single_trip_clusters\"] = get_result_labels_new(user_trip_df, filtered_trip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, there is no noise from oursim because all trips are in at least their cluster (a cluster of one)\n",
    "np.count_nonzero(user_trip_df.trip_cluster_dbscan == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim_single_trip_clusters == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim_single_trip_clusters < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_sim.delete_bins()\n",
    "above_cutoff_bins = curr_sim.bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_trip_df[\"trip_cluster_oursim_single_trip_clusters_above_cutoff\"] = get_result_labels_new(user_trip_df, filtered_trip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After finding the cutoff\n",
    "np.count_nonzero(user_trip_df.trip_cluster_dbscan == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim_single_trip_clusters_above_cutoff == -1), np.count_nonzero(user_trip_df.trip_cluster_oursim_single_trip_clusters_above_cutoff < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-spirit",
   "metadata": {},
   "source": [
    "### Spot checking the noise again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_noise_in_oursim = user_trip_df.query(\"(trip_cluster_oursim_single_trip_clusters_above_cutoff == -1) and (trip_cluster_dbscan != -1)\")\n",
    "extra_clusters = extra_noise_in_oursim.trip_cluster_dbscan.unique(); extra_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which of these clusters has > 1 trip (if any)\n",
    "two_trip_clusters = [c for c in extra_clusters if np.count_nonzero(user_trip_df.trip_cluster_dbscan == c) > 1]; two_trip_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = bre.Figure()\n",
    "for i, c in enumerate(two_trip_clusters):\n",
    "    print(i,c)\n",
    "    fig.add_subplot(3,2,i+1).add_child(folium.Map().add_child(get_geojson_for_trip_cluster(user_trip_df[user_trip_df.trip_cluster_dbscan == c])))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-priority",
   "metadata": {},
   "source": [
    "Most of the mismatches are for really small clusters, but cluster 0 is a really big one. Let's see how the similarity code dealt with it by visualizing the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_trip_df[user_trip_df.trip_cluster_dbscan == 0].trip_cluster_oursim_single_trip_clusters_above_cutoff.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,3,1).add_child(folium.Map().add_child(get_geojson_for_trip_cluster(user_trip_df.query(\"trip_cluster_dbscan == 0 and trip_cluster_oursim_single_trip_clusters_above_cutoff >= 0\"))))\n",
    "fig.add_subplot(1,3,2).add_child(folium.Map().add_child(get_geojson_for_trip_cluster(user_trip_df.query(\"trip_cluster_dbscan == 0 and trip_cluster_oursim_single_trip_clusters_above_cutoff == -1\"))))\n",
    "fig.add_subplot(1,3,3).add_child(folium.Map().add_child(get_geojson_for_trip_cluster(user_trip_df.query(\"trip_cluster_dbscan == 0 and trip_cluster_oursim_single_trip_clusters_above_cutoff == -2\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-measure",
   "metadata": {},
   "source": [
    "The DBSCAN algorithm is in fact more promiscuous the classic similarity algorithm in finding clusters, but the effect is minimal. We find only a handful of cases (at least for this user) (6 trips; 2%) where the similarity code did not find a cluster above the cutoff, but there was more than one trip found by DBSCAN. Looking at them exhaustively, the trip quality is not that great.\n",
    "\n",
    "Specifically, focusing on cluster 0, which had 34 trips in it, the similarity code found 7 trip cluster, a single trip cluser (below cutoff) and 25 \"too short\" trips. Looking at the 25 too short trips, most of them seem to be to a shopping center across the street from where the user lives and/or works. Would the user like to have these trips automatically labeled? You betcha!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-chemical",
   "metadata": {},
   "source": [
    "## Conclusion: we need to make significant changes in the way the similarity code is used for auto-labeling\n",
    "\n",
    "\n",
    "The similarity code was designed to surface common trips for user review and focus. It was not designed to automatically label trips. As we adapted it for this new use case, we need to make some simple changes to it. Note that the original goal was to surface the \"big ticket items\" of the trips, the new goal is to automatically handle the \"small ticket items\" so they don't bother the user.\n",
    "\n",
    "Concretely:\n",
    "\n",
    "- The filtering of short trips is unnecessary and sometimes incorrect, and it removes 49 trips (23%) of the trips already. See shopping center example above. While we may not need to surface these short shopping trips as part of the  larger tour model graph as a significant part of the user travel, it is particularly these trips that we should autolabel to reduce user burden.\n",
    "- The filtering of infrequent trips (below cutoff) from the model does not help at all. We instituted a cutoff because showing all these infrequent trips to the user would increase their cognitive burden. But we are now not showing these infrequent trips to the user, we are building a computer model from them. The computer can handle such a cognitive load without any problems. If the user took one trip and then took the same trip only a month later instead of every day, why would we not want to automatically label the trip? Again, because our goal is to take the cognitive burden off the user, it is better for the computer to remember the trip from the month ago and autolabel it.\n",
    "\n",
    "With these two changes, the results from this user indicate that the performance of the current similarity code can match or surpass the DBSCAN code. If these results hold against the multi-dataset, we can stick with the current similarity code and only make these changes to improve the overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-heater",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
