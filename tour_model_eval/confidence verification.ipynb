{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook investigates how well our prediction confidence matches reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T19:34:59.229998Z",
     "start_time": "2022-06-23T19:34:56.921810Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from data_wrangling import expand_df_dict, expand_df_list, add_top_pred\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.analysis.modelling.tour_model_first_only.data_preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T19:34:59.395223Z",
     "start_time": "2022-06-23T19:34:59.248880Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_users = esta.TimeSeries.get_uuid_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T15:59:12.405096Z",
     "start_time": "2022-06-24T15:57:34.361218Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up dataframe with our desires columns \n",
    "\n",
    "excluded_user_count = 0\n",
    "df_list = []\n",
    "\n",
    "for user in all_users:\n",
    "    trips = pp.read_data(user)\n",
    "    trip_df = pd.DataFrame(trips)\n",
    "    trip_df = expand_df_dict(trip_df, 'data')\n",
    "    if 'inferred_labels' in trip_df.columns:\n",
    "        trip_df = trip_df.drop(columns=['source', 'end_ts', 'end_local_dt', 'raw_trip', 'start_ts', \n",
    "                                        'start_local_dt', 'start_place', 'end_place', 'cleaned_trip'])\n",
    "        trip_df = expand_df_dict(trip_df, 'user_input').rename(columns={'mode_confirm':'mode_true', \n",
    "                                                                        'purpose_confirm':'purpose_true', \n",
    "                                                                        'replaced_mode':'replaced_true'})\n",
    "        try:\n",
    "            trip_df = expand_df_list(trip_df, 'inferred_labels')\n",
    "            trip_df = expand_df_dict(trip_df, 'inferred_labels')\n",
    "            trip_df = expand_df_dict(trip_df, 'labels').rename(columns={'mode_confirm':'mode_pred', \n",
    "                                                                        'purpose_confirm':'purpose_pred',\n",
    "                                                                        'replaced_mode':'replaced_pred'})\n",
    "        except Exception as e:\n",
    "            logging.debug('{} excluded'.format(user))\n",
    "            excluded_user_count += 1\n",
    "            logging.debug(str(e))\n",
    "            continue\n",
    "            \n",
    "       # indicates if the predicted label was chosen to be presented to user\n",
    "        trip_df = add_top_pred(trip_df, trip_id_column='_id', pred_conf_column='p')\n",
    "        df_list += [trip_df]\n",
    "\n",
    "    else:\n",
    "        logging.debug('{} excluded, no inferred labels'.format(user))\n",
    "        excluded_user_count += 1\n",
    "\n",
    "all_trips = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T00:09:05.631726Z",
     "start_time": "2022-06-24T00:09:05.620353Z"
    }
   },
   "outputs": [],
   "source": [
    "print('{} users included, {} users excluded from dataset with {} total users'.format(len(all_users) - excluded_user_count, excluded_user_count, len(all_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:38:15.180413Z",
     "start_time": "2022-06-24T22:38:14.324503Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_trips[['_id','user_id', 'mode_true', 'purpose_true', 'mode_pred', 'purpose_pred', 'p', 'top_pred']].sort_values('_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:43:43.563594Z",
     "start_time": "2022-06-24T22:43:42.931294Z"
    }
   },
   "outputs": [],
   "source": [
    "all_trips.groupby(['_id','user_id', 'mode_pred'], as_index=False).agg({'p':sum, 'top_pred':any})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with the raw trip dictionaries to make sure data was added correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T15:49:12.310609Z",
     "start_time": "2022-06-24T15:49:09.261356Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trips = pp.read_data(all_users[1])\n",
    "# for t in trips:\n",
    "#     for e in t:\n",
    "#         print(e)\n",
    "#         print(t[e])\n",
    "#         print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:45:48.840851Z",
     "start_time": "2022-06-24T22:45:48.827335Z"
    }
   },
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "\n",
    "def conf_verification(trips_df, label_type, n, user=None, only_top_pred=False):\n",
    "    \"\"\" Returns a data frame with n rows showing the predicted and actual accuracy for n quantiles. \n",
    "    \n",
    "        Args:\n",
    "            trips_df: dataframe containing true labels, predicted labels, and predicted confidence\n",
    "                Should have the columns '<label_type>_true', '<label_type>_pred', 'p'\n",
    "            label_type (str): 'mode', 'purpose', or 'replaced'\n",
    "            n (int): number of quantiles\n",
    "            user (str or UUID): UUID, if we only want the confidence table for a single user.\n",
    "            top_pred (bool): whether or not we only look at top predictions, or look at all predictions\n",
    "                (including alternative labels that may not have been suggested to the user)\n",
    "    \"\"\"\n",
    "    conf_quantiles = []\n",
    "    \n",
    "    if user:\n",
    "        if type(user) == str:\n",
    "            user = UUID(user)\n",
    "        assert type(user) == UUID\n",
    "        trips_df = trips_df[trips_df.user_id==user]\n",
    "\n",
    "    # ignore trips that don't have confirmed user input\n",
    "    trips_df = trips_df.dropna(subset=[label_type + '_true'])\n",
    "    \n",
    "    # merge rows with duplicates of this label_type \n",
    "    # e.g. if a trip has two predictions, (car, work, bike) at 50% and \n",
    "    # (car, shopping, no travel) at 30%, and we want to get the confidence \n",
    "    # of mode labels, we want to combine the confidence of these two predictions \n",
    "    # to yield the true confidence for 'car'\n",
    "    trips_df = trips_df.groupby(['_id','user_id', label_type + '_true', label_type + '_pred'], as_index=False).agg({'p':sum, 'top_pred':any})\n",
    "    \n",
    "    if only_top_pred:\n",
    "        trips_df = trips_df[(trips_df.top_pred)]\n",
    "                                      \n",
    "    for i in range(n):\n",
    "        quantile_min = i / n\n",
    "        quantile_max = (i + 1)/n\n",
    "        \n",
    "        # get trips in this quantile\n",
    "        trips_in_range = trips_df[(trips_df['p'] >= quantile_min) & (trips_df['p'] < quantile_max)]\n",
    "        \n",
    "        num_predictions = len(trips_in_range)\n",
    "        num_correct = len(trips_in_range[trips_in_range[label_type + '_true'] == trips_in_range[label_type + '_pred']])\n",
    "\n",
    "        conf_quantiles.append(['{:.2f} - {:.2f}'.format(quantile_min, quantile_max), num_predictions, num_correct])\n",
    "\n",
    "    columns=['stated confidence range', 'num_predictions', 'num_correct']\n",
    "    conf_df = pd.DataFrame(conf_quantiles, columns=columns)\n",
    "    conf_df['accuracy'] = np.round(conf_df['num_correct'] / conf_df['num_predictions'], 3)\n",
    "\n",
    "    return conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:45:58.053384Z",
     "start_time": "2022-06-24T22:45:57.515046Z"
    }
   },
   "outputs": [],
   "source": [
    "print('confidence for mode predictions')\n",
    "conf_verification(all_trips, 'mode', 10, only_top_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:46:37.393339Z",
     "start_time": "2022-06-24T22:46:36.756459Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('confidence for purpose predictions')\n",
    "conf_verification(all_trips, 'purpose', 10, only_top_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:46:48.477046Z",
     "start_time": "2022-06-24T22:46:47.994559Z"
    }
   },
   "outputs": [],
   "source": [
    "print('confidence for purpose predictions')\n",
    "conf_verification(all_trips, 'replaced', 10, only_top_pred=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data for 38 users who had received label suggestions. We evaluate *all* of our label predictions for these users, including backup/alternative predictions that may not have been shown. \n",
    "\n",
    "This is pretty good! The confidences are fairly realistic, though there is some variability (we are underconfident for some and overconfident for some). \n",
    "\n",
    "Now I want to see what happens if we break this up into 0.05 increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:48:47.720016Z",
     "start_time": "2022-06-24T22:48:47.272443Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_verification(all_trips, 'mode', 20, only_top_pred=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that our confidence is pretty realistic makes intuitive sense â€“ imagine if we threw all trips from every single user into a singular giant cluster, gave everybody the same prediction based on the probability distribution of that giant cluster, and set the confidence to the the frequency of that label. The confidence would be pretty realistic if people continued taking the same kinds of trips. So it's good that our confidence is fairly realistic but the more important part is that we want more predictions to be at the higher confidence levels. Accurately stating low confidence in a bad prediction doesn't help the users much. \n",
    "\n",
    "I wonder how it will look if we only looked at the confidence of the top prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:48:54.732146Z",
     "start_time": "2022-06-24T22:48:54.312657Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_verification(all_trips, 'mode', 10, only_top_pred=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the confidence for some random users to see how much variation there is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:51:39.460321Z",
     "start_time": "2022-06-24T22:51:39.380873Z"
    }
   },
   "outputs": [],
   "source": [
    "random_user = np.random.choice(all_users)\n",
    "# most users actually didn't have predicted labels so I had to run the RNG a bunch of\n",
    "# times until I actually got a valid user\n",
    "print(random_user)\n",
    "conf_verification(all_trips, 'mode', 10, user=random_user, only_top_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T22:51:39.931501Z",
     "start_time": "2022-06-24T22:51:39.829737Z"
    }
   },
   "outputs": [],
   "source": [
    "random_user = np.random.choice(all_users)\n",
    "# again: most users actually didn't have predicted labels so I had to run the RNG a bunch\n",
    "# of times until I actually got a valid user\n",
    "print(random_user)\n",
    "conf_verification(all_trips, 'mode', 10, user=random_user, only_top_pred=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purely anecdotally, it appears that we tend to be slightly underconfident in the trips that we say we are 50-80% confident in. \n",
    "\n",
    "Also, there appears to be more variability in confidence agreement for individual users than for the entire dataset, which is expected due to smaller sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I went back and updated the code to address the following comments.)\n",
    "\n",
    "~~I have a hypothesis for why our confidence was underestimated: we provide confidences for an entire tuple rather on a mode/purpose/replaced mode basis. Consider an example where we have the following predictions:~~\n",
    "\n",
    "- ~~prediction 1, 50% confidence: mode car, purpose recreation, replaced walk~~\n",
    "- ~~prediction 2, 30% confidence: mode car, purpose shopping, replaced bike~~\n",
    "\n",
    "~~Notice that the overall accuracy for the mode car prediction is 80%, even though we listed it as two separate predictions of 50% and 30%. Thus, in the way that I'm currently assessing confidence, these would be treated as two separate predictions, and be placed in the 0.3-0.4 and 0.5-0.6 deciles, even though it should be a single prediction in the 0.8-0.9 decile. ~~\n",
    "\n",
    "~~This also explains why we're 'overconfident' for lower deciles but it becomes more realistic for higher deciles: at higher deciles, it is less likely that I counted a prediction in the wrong decile (there are simply fewer deciles above it that it could belong in). ~~\n",
    "\n",
    "~~It is slightly more concerning that we were overconfident for a few users, because that can't be explained by this bug. (Well, maybe its possible that fixing this bug and shifting predictions to their correct decile will smooth things out)~~\n",
    "\n",
    "There are two things I can do: \n",
    "1. ~~fix the confidence assessment to account for this~~ (yep, did this)\n",
    "2. update label predictions to have separated confidences for mode, purpose, and replaced\n",
    "\n",
    "Option 2 is going to take more work with refactoring the code base, but it also seems like the better long-term option, since we plan to merge label assist with sensed-mode in the ensemble algorithm (which means some trips may have predicted mode but no predicted purpose), and we can try to extend the label assist algorithm to predict a trip's purpose even when the start location is not known (which would not yield a predicted mode or replaced mode). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thought: because there might be correlation between mode and purpose, it may make sense to assess the confidence at the tuple level. (The interesting thing about this tuple-packaging is that we may end up with a mode label, for example, that is the suggested label even if it does not have the highest cumulative confidence.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
